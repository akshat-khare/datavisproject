%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
 \ifdefined\DeclareUnicodeCharacterAsOptional
  \DeclareUnicodeCharacter{"00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{"2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{"2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{"2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{"251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{"2572}{\textbackslash}
 \else
  \DeclareUnicodeCharacter{00A0}{\nobreakspace}
  \DeclareUnicodeCharacter{2500}{\sphinxunichar{2500}}
  \DeclareUnicodeCharacter{2502}{\sphinxunichar{2502}}
  \DeclareUnicodeCharacter{2514}{\sphinxunichar{2514}}
  \DeclareUnicodeCharacter{251C}{\sphinxunichar{251C}}
  \DeclareUnicodeCharacter{2572}{\textbackslash}
 \fi
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Bjarne]{fncychap}
\usepackage[dontkeepoldnames]{sphinx}

\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}
\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\addto\captionsenglish{\renewcommand{\figurename}{Fig.}}
\addto\captionsenglish{\renewcommand{\tablename}{Table}}
\addto\captionsenglish{\renewcommand{\literalblockname}{Listing}}

\addto\captionsenglish{\renewcommand{\literalblockcontinuedname}{continued from previous page}}
\addto\captionsenglish{\renewcommand{\literalblockcontinuesname}{continues on next page}}

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}


\title{Twitter Analytics Documentation}
\date{Jun 28, 2018}
\release{0.1}
\author{Deepak Saini, Abhishek Gupta}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex

\begin{document}

\maketitle
\sphinxtableofcontents
\phantomsection\label{\detokenize{index::doc}}


Twitter generates millions of tweets each day. A vast amount of information is hence available for different kinds of analyses. However, this also brings up three challenges. First, the system needs to be efficient enough to be able to absorb data at such high rates. Second, to be able to perform complex analysis on this data, it needs an appropriate representation in the database. Third, the system needs to provide an intuitive abstraction of the data so that the users can specify various complex analyses without having to write complex programs using different software tools. This will allow even slightly non-technical users to be able to use the system.

In this demonstration, we introduce a system that tries to tackle the above challenges. The system uses a couple of data stores - Neo4j and MongoDB - to persist the data in a way that allows complex historical queries to be answered efficiently. It also provides an intuitive abstract representation of this data, allowing users to formulate complex queries. Also, running on streaming data, the system provides an abstraction which allows the user to specify real time events in the stream, for which he wishes to be notified. We demonstrate both of these abstractions using an example of each, on real world data.

NOTE : For all experiments mentioned in this document, we use a Intel i7 processor with 16 GB RAM.


\chapter{Introduction to twitter analytics system}
\label{\detokenize{introduction:welcome-to-twitter-analytics-documentation}}\label{\detokenize{introduction:introduction-to-twitter-analytics-system}}\label{\detokenize{introduction::doc}}
Nowadays social media generates huge amount of data which can be used to infer a number of trending social events and to understand the connections between different entities. One such useful platform is Twitter where users can follow other users and tweet on any topic, tagging it with hashtags, mentioning other users or using URLs. This data can be spatially represented as a network of entities (like users, tweets, hashtags and URLs) which are interconnected in complex ways. A number of programming tools are often used by developers to perform various tasks on a subset of this data.

However, in reality, this data is not static with time, giving it a temporal dimension as well. The system thus needs to capture both the spatially and temporally evolving aspects of this network in its database schema. The same abstraction also needs to be extended to the query specification process, making it easier to specify queries without writing queries in complex database specific languages. Hence, instead of limiting the users to a limited set of APIs, this system would allow any complex query to be specified.

Our system continuously streams tweets from the Twitter Streaming API, persists them into a database schema compliant with the above needs and gives users a couple of abstractions to work with. The first abstraction is over the live stream of tweets allowing them to detect custom live events (Eg. finding hashtags going viral) in the stream and get notified about them. The second abstraction is over the historical view over the data stored from the stream. The second abstraction looks at the data as a network of users, tweets and their attributes, along with the network’s temporal dimension, allowing users to specify complex queries in an abstract way.

Together, these two abstractions would provide an intuitive analytics platform for the users.


\section{Major parts of the system}
\label{\detokenize{introduction:major-parts-of-the-system}}
\noindent\sphinxincludegraphics{{5}.png}

Lets begin by describing the major components of the system.
\begin{itemize}
\item {} 
The streaming tweet collector: We have a connection to the twitter streaming API which keeps on collecting tweets from the twitter pipe. For more details refer to the section on twitter stream.

\item {} 
The alert generation system: The collected tweets are pushed to \sphinxstylestrong{Apache Kafka} for downstream processes. This tweet stream is processed by \sphinxstylestrong{Apache Flink}, which is an open-source, distributed and high-performance stream processing framework, to look for user specified alerts in the tweet stream.

\item {} 
The datastores: The stream is also persisted in a couple of data stores to make queries later. \sphinxstylestrong{Neo4j}, which is a graph database management system supported by the query language Cypher, is used to store network based information from these tweets. \sphinxstylestrong{MongoDB}, which is a document oriented database, is used to store document based information to answer simpler aggregate queries.

\item {} 
The dashboard: These alerts and queries are then accessible to the user from a web application based dashboard. Please refer to the section  {\hyperref[\detokenize{dashboard_website:dashboard-website}]{\sphinxcrossref{\DUrole{std,std-ref}{Dashboard Website}}}} in which we explain the functionalities of the system through use cases.

\end{itemize}


\chapter{Read data from Twitter API}
\label{\detokenize{twitter_stream:read-data-from-twitter-api}}\label{\detokenize{twitter_stream::doc}}
There are a couple of ways to read data from the Twitter APIs:
\begin{itemize}
\item {} 
User Timeline: Fetching the timeline (all tweets) of a given user till that time. The Twitter API end-point for this is GET statuses/user\_timeline

\item {} 
Stream Sample: Streaming a random 1\% sample of all live tweets. The Twitter API end-point for the same is GET statuses/sample.

\end{itemize}

If you wish to gather data of a specific set of users, then you have to use the User Timeline API, otherwise for live stream use the Stream Sample API. We experimented with both the techniques.

To have access to Twitter APIs, you need to create an account on apps.twitter.com to use their OAuth based authorization system. Details for the same can be found on this \sphinxhref{https://developer.twitter.com/en/docs/basics/getting-started}{link}. This will generate a unique set of Access Token, Access Secret, Consumer Key and Consumer Secret for you, which need to be used while making the API calls. We made use of Python Twitter Tools library which exposes Python functions which make the API call for us based on the parameters.


\section{User Timeline API}
\label{\detokenize{twitter_stream:user-timeline-api}}
For a given user, you can find the following:
\begin{itemize}
\item {} 
User information: Eg. no. of tweets, no. of followers, no. of friends, no. of likes, location etc.

\item {} 
All tweets by the user till that point of time.

\item {} 
All friends and followers of that user at that point of time.

\end{itemize}

Rate Limit: Each API endpoint has its own rate limit which limits the number of calls you can make to Twitter for that API in a period of 15 minutes. The rate limit can be checked \sphinxhref{https://developer.twitter.com/en/docs/basics/rate-limits}{here}. You need to adhere to these rate limits, else your account may be blacklisted. We do this in our application as follows: For each API endpoint, we maintain an array which keeps the history of number of calls made in each minute in the past 15 minutes. For each request of an API call, we check the array, delete entries older than 15 minutes and keep checking the total number of requests in past 15 minutes until it is less than the rate limit threshold. Only then we make the API call.

Incremental data: You may need to periodically make calls to the Twitter API to get the new tweets that were tweeted by the users since the last time you called the API. Twitter provides a mechanism to do this by using the parameter “since\_id” in the statuses/user\_timeline API call. The API returns tweets that have an id \textgreater{} since\_id. Thus you can keep store of the maximum tweet id that you have seen for each user and make the next API call using that as the value of since\_id to get the new tweets.

Refer to the file: ‘Read Twitter Stream/main.py’ for the code. The main function expects a file containing a list of user screen names separated by new lines. It generates a folder the data as mentioned in the main function. Each time you run the file, it accumulates the new data in this directory.


\section{Stream Sample API}
\label{\detokenize{twitter_stream:stream-sample-api}}
The end-point GET statuses/sample is free, however it returns only about 1\% of the actual stream. You can buy the enterprise version called Decahose to have access to 10\% of the stream.

Batch writes: In order to speed up the process of persisting the tweets from memory to disk, we make use of batch writes. Our streaming application will keep buffering tweets till the batch size is reached. Once the the batch size is reached, it flushes all the buffered tweets to the current tweet file. Additionally, it keeps track of number of tweets written to the current tweet file and once it exceeds a threshold, it starts with a new file and starts flushing to it. This will prevent a single file from becoming too big in size.

Refer to the file ‘Read Twitter Stream/streaming.py’ for the code. The code will write the data in the same directory, flushing the data periodically to a file. After a threshold number of tweets have been written to the current output file, it generates a new file and starts flushing the tweets to it. This will prevent a single file from becoming too big in size.


\section{User Timeline API Code Documentation}
\label{\detokenize{twitter_stream:user-timeline-api-code-documentation}}
Here we provide a documentation of the code.
\phantomsection\label{\detokenize{twitter_stream:module-userstimeline}}\index{userstimeline (module)}
Module to fetch data of a specified list of users. Data includes user’s profile information, all tweets on
user’s timeline till now and list of ids of user’s followers and friends.

In the main function, configure the path of the directory of the data folder to be created and the path of a file containing
a list of user screen names separated by new lines. It generates a folder for the data as specified in the main function.
Each time you run the file, it accumulates the new data in this directory.

Create a file named SECRETS which contains your Twitter OAuth related keys in the following order, separated by new lines:
\textless{}ACCESS\_TOKEN\textgreater{}
\textless{}ACCESS\_SECRET\textgreater{}
\textless{}CONSUMER\_KEY\textgreater{}
\textless{}CONSUMER\_SECRET\textgreater{}

\sphinxstylestrong{Running the code}:
\begin{quote}
\begin{itemize}
\item {} 
First ensure Python Twitter Tools is installed. (\sphinxurl{https://github.com/sixohsix/twitter})

\item {} 
Before running, you may want to change the name of the file (containing the user screen names)
in the main function. That file should contain one screen name in each line.

\end{itemize}

\sphinxstyleemphasis{Command to run}:
\sphinxcode{python main.py}
\end{quote}
\index{DateTimeEncoder (class in userstimeline)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:userstimeline.DateTimeEncoder}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{userstimeline.}\sphinxbfcode{DateTimeEncoder}}{\emph{*}, \emph{skipkeys=False}, \emph{ensure\_ascii=True}, \emph{check\_circular=True}, \emph{allow\_nan=True}, \emph{sort\_keys=False}, \emph{indent=None}, \emph{separators=None}, \emph{default=None}}{}
Bases: \sphinxcode{json.encoder.JSONEncoder}
\index{default() (userstimeline.DateTimeEncoder method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:userstimeline.DateTimeEncoder.default}}\pysiglinewithargsret{\sphinxbfcode{default}}{\emph{o}}{}
\end{fulllineitems}

\index{encode() (userstimeline.DateTimeEncoder method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:userstimeline.DateTimeEncoder.encode}}\pysiglinewithargsret{\sphinxbfcode{encode}}{\emph{o}}{}
Return a JSON string representation of a Python data structure.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{json}\PYG{n+nn}{.}\PYG{n+nn}{encoder} \PYG{k}{import} \PYG{n}{JSONEncoder}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{JSONEncoder}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{encode}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{foo}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{bar}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{baz}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{g+go}{\PYGZsq{}\PYGZob{}\PYGZdq{}foo\PYGZdq{}: [\PYGZdq{}bar\PYGZdq{}, \PYGZdq{}baz\PYGZdq{}]\PYGZcb{}\PYGZsq{}}
\end{sphinxVerbatim}

\end{fulllineitems}

\index{item\_separator (userstimeline.DateTimeEncoder attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:userstimeline.DateTimeEncoder.item_separator}}\pysigline{\sphinxbfcode{item\_separator}\sphinxbfcode{ = ', '}}
\end{fulllineitems}

\index{iterencode() (userstimeline.DateTimeEncoder method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:userstimeline.DateTimeEncoder.iterencode}}\pysiglinewithargsret{\sphinxbfcode{iterencode}}{\emph{o}, \emph{\_one\_shot=False}}{}
Encode the given object and yield each string
representation as available.

For example:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{chunk} \PYG{o+ow}{in} \PYG{n}{JSONEncoder}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{iterencode}\PYG{p}{(}\PYG{n}{bigobject}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{mysocket}\PYG{o}{.}\PYG{n}{write}\PYG{p}{(}\PYG{n}{chunk}\PYG{p}{)}
\end{sphinxVerbatim}

\end{fulllineitems}

\index{key\_separator (userstimeline.DateTimeEncoder attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:userstimeline.DateTimeEncoder.key_separator}}\pysigline{\sphinxbfcode{key\_separator}\sphinxbfcode{ = ': '}}
\end{fulllineitems}


\end{fulllineitems}

\index{UserTimelineAPI (class in userstimeline)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:userstimeline.UserTimelineAPI}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{userstimeline.}\sphinxbfcode{UserTimelineAPI}}{\emph{data\_directory}}{}
Bases: \sphinxcode{object}

Class to fetch data of a specified list of users.
\index{clear\_everyting() (userstimeline.UserTimelineAPI method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:userstimeline.UserTimelineAPI.clear_everyting}}\pysiglinewithargsret{\sphinxbfcode{clear\_everyting}}{}{}
Clears the data folder

\end{fulllineitems}

\index{fetch\_persist\_friends\_and\_followers() (userstimeline.UserTimelineAPI method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:userstimeline.UserTimelineAPI.fetch_persist_friends_and_followers}}\pysiglinewithargsret{\sphinxbfcode{fetch\_persist\_friends\_and\_followers}}{\emph{user\_screen\_names}, \emph{time}}{}
Fetches and persists users’ friends and followers
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{user\_screen\_names} \textendash{} list of screen names of users

\item {} 
\sphinxstyleliteralstrong{time} \textendash{} wall clock time when this file started running

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{fetch\_persist\_tweets() (userstimeline.UserTimelineAPI method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:userstimeline.UserTimelineAPI.fetch_persist_tweets}}\pysiglinewithargsret{\sphinxbfcode{fetch\_persist\_tweets}}{\emph{user\_screen\_names}, \emph{time}, \emph{type\_}}{}
Fetches and persists tweets (excluding tweets already persisted)
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{user\_screen\_names} \textendash{} list of screen names of users

\item {} 
\sphinxstyleliteralstrong{time} \textendash{} wall clock time when this file started running

\item {} 
\sphinxstyleliteralstrong{type} \textendash{} one of ‘tweets’ or ‘favourites’ to fetch user’s own tweets or favorited tweets respectively

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{fetch\_persist\_users() (userstimeline.UserTimelineAPI method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:userstimeline.UserTimelineAPI.fetch_persist_users}}\pysiglinewithargsret{\sphinxbfcode{fetch\_persist\_users}}{\emph{user\_screen\_names}, \emph{time}}{}
Fetches and persists user information (calling this multiple times will keep adding new entries so that you can compare over time)
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{user\_screen\_names} \textendash{} list of screen names of users

\item {} 
\sphinxstyleliteralstrong{time} \textendash{} wall clock time when this file started running

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{my\_favourites\_fetcher() (userstimeline.UserTimelineAPI method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:userstimeline.UserTimelineAPI.my_favourites_fetcher}}\pysiglinewithargsret{\sphinxbfcode{my\_favourites\_fetcher}}{\emph{retry\_no=0}, \emph{**kwargs}}{}
Waits until rate limit is clear and then calls the Twitter API to fetch user’s favorited tweets.
Retries 3 times in case of exceptions.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{retry\_no} \textendash{} Number of retries done till now in case of exceptions

\item {} 
\sphinxstyleliteralstrong{kwargs} \textendash{} Contains screen\_name, count (maximum 200 allowed), since\_id (minimum id of tweet to look for), max\_id (maximum id of tweet to look for)

\end{itemize}

\item[{Returns}] \leavevmode
A list of tweet dictionaries

\end{description}\end{quote}

\end{fulllineitems}

\index{my\_followers\_fetcher() (userstimeline.UserTimelineAPI method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:userstimeline.UserTimelineAPI.my_followers_fetcher}}\pysiglinewithargsret{\sphinxbfcode{my\_followers\_fetcher}}{\emph{retry\_no=0}, \emph{**kwargs}}{}
Waits until rate limit is clear and then calls the Twitter API to fetch user’s followers’ ids.
Retries 3 times in case of exceptions.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{retry\_no} \textendash{} Number of retries done till now in case of exceptions

\item {} 
\sphinxstyleliteralstrong{kwargs} \textendash{} Contains screen\_name, cursor (used for paginated results, -1 to fetch the latest batch)

\end{itemize}

\item[{Returns}] \leavevmode
A dictionary containing ‘ids’ (ids of followers) and ‘next\_cursor’ (cursor of next batch)

\end{description}\end{quote}

\end{fulllineitems}

\index{my\_friends\_fetcher() (userstimeline.UserTimelineAPI method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:userstimeline.UserTimelineAPI.my_friends_fetcher}}\pysiglinewithargsret{\sphinxbfcode{my\_friends\_fetcher}}{\emph{retry\_no=0}, \emph{**kwargs}}{}
Waits until rate limit is clear and then calls the Twitter API to fetch user’s friends’ ids.
Retries 3 times in case of exceptions.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{retry\_no} \textendash{} Number of retries done till now in case of exceptions

\item {} 
\sphinxstyleliteralstrong{kwargs} \textendash{} Contains screen\_name, cursor (used for paginated results, -1 to fetch the latest batch)

\end{itemize}

\item[{Returns}] \leavevmode
A dictionary containing ‘ids’ (ids of friends) and ‘next\_cursor’ (cursor of next batch)

\end{description}\end{quote}

\end{fulllineitems}

\index{my\_tweet\_fetcher() (userstimeline.UserTimelineAPI method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:userstimeline.UserTimelineAPI.my_tweet_fetcher}}\pysiglinewithargsret{\sphinxbfcode{my\_tweet\_fetcher}}{\emph{retry\_no=0}, \emph{**kwargs}}{}
Waits until rate limit is clear and then calls the Twitter API to fetch user’s tweets.
Retries 3 times in case of exceptions.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{retry\_no} \textendash{} Number of retries done till now in case of exceptions

\item {} 
\sphinxstyleliteralstrong{kwargs} \textendash{} Contains screen\_name, count (maximum 200 allowed), since\_id (minimum id of tweet to look for), max\_id (maximum id of tweet to look for)

\end{itemize}

\item[{Returns}] \leavevmode
A list of tweet dictionaries

\end{description}\end{quote}

\end{fulllineitems}

\index{my\_user\_fetcher() (userstimeline.UserTimelineAPI method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:userstimeline.UserTimelineAPI.my_user_fetcher}}\pysiglinewithargsret{\sphinxbfcode{my\_user\_fetcher}}{\emph{retry\_no=0}, \emph{**kwargs}}{}
Waits until rate limit is clear and then calls the Twitter API to fetch the given users’ info.
Retries 3 times in case of exceptions.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{retry\_no} \textendash{} Number of retries done till now in case of exceptions

\item {} 
\sphinxstyleliteralstrong{kwargs} \textendash{} Contains screen\_name which is comma separated list of screen names

\end{itemize}

\item[{Returns}] \leavevmode
A list of user info dictionaries

\end{description}\end{quote}

\end{fulllineitems}

\index{wait\_for\_rate\_limit() (userstimeline.UserTimelineAPI method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:userstimeline.UserTimelineAPI.wait_for_rate_limit}}\pysiglinewithargsret{\sphinxbfcode{wait\_for\_rate\_limit}}{\emph{type\_}}{}
Common function for all API calls that waits (sleeps) until the restriction of rate limiting for that type is clear.
For each request of each API call, it checks the corresponding array in COUNTS\_DICT, deletes entries older than
WINDOW\_LEN minutes and keeps checking the total no of requests in past WINDOW\_LEN minutes until it is less
than the rate limit threshold set in COUNTS\_LIMIT. Sleeps in between.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{type} \textendash{} The type of API call. One of ‘USERS\_LOOKUP’, ‘TWEETS’, ‘FOLLOWERS’, ‘FRIENDS’, ‘FAVOURITES’.

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{extract\_hash\_tags() (in module userstimeline)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:userstimeline.extract_hash_tags}}\pysiglinewithargsret{\sphinxcode{userstimeline.}\sphinxbfcode{extract\_hash\_tags}}{\emph{screen\_name}, \emph{date\_start}, \emph{date\_end}}{}
\end{fulllineitems}

\index{get\_user\_screen\_names() (in module userstimeline)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:userstimeline.get_user_screen_names}}\pysiglinewithargsret{\sphinxcode{userstimeline.}\sphinxbfcode{get\_user\_screen\_names}}{\emph{filename}}{}
\end{fulllineitems}

\index{plot\_user\_field() (in module userstimeline)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:userstimeline.plot_user_field}}\pysiglinewithargsret{\sphinxcode{userstimeline.}\sphinxbfcode{plot\_user\_field}}{\emph{screen\_name}, \emph{date\_start}, \emph{date\_end}, \emph{field\_names}}{}
\end{fulllineitems}



\section{Stream Sample API Code Documentation}
\label{\detokenize{twitter_stream:stream-sample-api-code-documentation}}
Here we provide a documentation of the code.
\phantomsection\label{\detokenize{twitter_stream:module-streaming}}\index{streaming (module)}
Module to stream 1\% sample of tweets using Twitter’s Streaming API. The code will write the data in the same directory.

Create a file named SECRETS which contains your Twitter OAuth related keys in the following order, separated by new lines:
\textless{}ACCESS\_TOKEN\textgreater{}
\textless{}ACCESS\_SECRET\textgreater{}
\textless{}CONSUMER\_KEY\textgreater{}
\textless{}CONSUMER\_SECRET\textgreater{}

\sphinxstylestrong{Running the code}:
\begin{quote}
\begin{itemize}
\item {} 
First ensure Python Twitter Tools is installed. (\sphinxurl{https://github.com/sixohsix/twitter})

\item {} 
Before running, you may set the following parameters inside the file:
\begin{itemize}
\item {} 
tweet\_count = 1000000000 \# total number of tweets to read

\item {} 
batch\_size = 10000 \# how many tweets to write together in 1 go

\item {} 
file\_tweet\_count = 500000 \# max no of tweets to write in 1 file

\end{itemize}

\end{itemize}

\sphinxstyleemphasis{Command to run}:
\sphinxcode{python streaming.py}
\end{quote}
\index{Logger (class in streaming)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:streaming.Logger}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{streaming.}\sphinxbfcode{Logger}}{\emph{batch\_size}, \emph{file\_tweet\_count}}{}
Bases: \sphinxcode{object}

Class to periodically write tweets to files and to log anything (automatically timestamped).
The function write\_tweet will keep buffering tweets till batch\_size is reached. Once the batch size
is reached, it flushes all the buffered tweets to the current tweet\_file\_name. Additionally, it keeps
track of number of tweets written to the current file and once it exceeds the threshold file\_tweet\_count,
it starts with a new tweet\_file and starts flushing to it. This will prevent a single file from becoming
too big in size.
\index{configure\_new\_file() (streaming.Logger method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:streaming.Logger.configure_new_file}}\pysiglinewithargsret{\sphinxbfcode{configure\_new\_file}}{}{}
Changes the configuration to start logging and writing tweets to new files (filenames will contain timestamp).
Also puts a ‘{[}‘ at the beginning of the tweet file to start a new list.

\end{fulllineitems}

\index{finish\_current\_file() (streaming.Logger method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:streaming.Logger.finish_current_file}}\pysiglinewithargsret{\sphinxbfcode{finish\_current\_file}}{}{}
Finshes the current tweet file by ending the list with a ‘{]}’.

\end{fulllineitems}

\index{log() (streaming.Logger method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:streaming.Logger.log}}\pysiglinewithargsret{\sphinxbfcode{log}}{\emph{s}}{}
Logs the string supplied as argument to the current log file.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{s} \textendash{} The string to log

\end{description}\end{quote}

\end{fulllineitems}

\index{write\_tweet() (streaming.Logger method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:streaming.Logger.write_tweet}}\pysiglinewithargsret{\sphinxbfcode{write\_tweet}}{\emph{tweet}}{}
Adds the tweet to the buffer. If buffer is filled, then flushes the buffer to the current tweet file.
If the current tweet file exceeds the tweet count threshold, finishes the current file and a configures a new one.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{tweet} \textendash{} The tweet to write to file

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{signal\_handler() (in module streaming)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{twitter_stream:streaming.signal_handler}}\pysiglinewithargsret{\sphinxcode{streaming.}\sphinxbfcode{signal\_handler}}{\emph{signal}, \emph{frame}}{}
\end{fulllineitems}



\chapter{Putting tweets to Kafka}
\label{\detokenize{kafka:putting-tweets-to-kafka}}\label{\detokenize{kafka::doc}}
The tweets read from the Twitter API are written to files on the disk for any further processing. These tweets are then read from the files and put on a Kafka topic (“tweets\_topic”) for the downstream processes. Ideally, in a running application, the tweets from the API should directly be put on the Kafka topic without writing to the files. However, collecting data once in the files helps in easy testing of different components.

We now provide the documentation for the application that reads tweets from the files and puts them to a Kafka topic.


\section{Kafka Tweet Producer Documentation}
\label{\detokenize{kafka:kafka-tweet-producer-documentation}}
Here we provide a documentation of the Kafka Tweet Producer.
\phantomsection\label{\detokenize{kafka:module-kafka_tweets_producer}}\index{kafka\_tweets\_producer (module)}
Module to read tweets from files and post them to a Kafka topic (currently “tweets\_topic”) for any downstream processes.

\begin{sphinxadmonition}{note}{Note:}
Currently, only the Flink applications are reading tweets from this Kafka topic.
In our proposed architecture, MongoDB and Neo4j ingestion applications should also read tweets from this Kafka topic.
However this is not integrated yet, hence MongoDB and Neo4j applications currenlty read tweets from files themselves.
This should be integrated as such.
\end{sphinxadmonition}
\index{Producer (class in kafka\_tweets\_producer)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kafka:kafka_tweets_producer.Producer}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{kafka\_tweets\_producer.}\sphinxbfcode{Producer}}{\emph{tweet\_folder\_name}, \emph{max\_q\_size}}{}
Bases: \sphinxcode{object}

Class that on initialization, spawns 2 threads. One thread keeps reading tweets from the files and puts them to a queue.
Second thread keeps reading tweets from the queue and puts them to the Kafka topic.

\end{fulllineitems}

\index{ServiceExit}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kafka:kafka_tweets_producer.ServiceExit}}\pysigline{\sphinxbfcode{exception }\sphinxcode{kafka\_tweets\_producer.}\sphinxbfcode{ServiceExit}}
Bases: \sphinxcode{Exception}

Custom exception which is used to trigger the clean exit
of all running threads and the main program.
\index{args (kafka\_tweets\_producer.ServiceExit attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kafka:kafka_tweets_producer.ServiceExit.args}}\pysigline{\sphinxbfcode{args}}
\end{fulllineitems}

\index{with\_traceback() (kafka\_tweets\_producer.ServiceExit method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kafka:kafka_tweets_producer.ServiceExit.with_traceback}}\pysiglinewithargsret{\sphinxbfcode{with\_traceback}}{}{}
Exception.with\_traceback(tb) \textendash{}
set self.\_\_traceback\_\_ to tb and return self.

\end{fulllineitems}


\end{fulllineitems}

\index{service\_shutdown() (in module kafka\_tweets\_producer)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{kafka:kafka_tweets_producer.service_shutdown}}\pysiglinewithargsret{\sphinxcode{kafka\_tweets\_producer.}\sphinxbfcode{service\_shutdown}}{\emph{signum}, \emph{frame}}{}
\end{fulllineitems}



\chapter{Ingesting data into Neo4j}
\label{\detokenize{neo4j_data_ingestion:ingesting-data-into-neo4j}}\label{\detokenize{neo4j_data_ingestion::doc}}

\section{Data stored in neo4j}
\label{\detokenize{neo4j_data_ingestion:data-stored-in-neo4j}}
Our aim in the project is to capture the dynamics of an evolving social network. These dynamics can be a combination of :
\begin{itemize}
\item {} 
Spatial dynamics : captured by network based information.

\item {} 
Temporal dynamics : The spatial information present at some interval of time in the past. This makes sense as the network keeps on changing and the user might want to see the state of some part of it at some point in past.

\end{itemize}

We store the complete twitter network in graph database neo4j.

For sake of understanding, lets divide the complete network into three parts:
\begin{itemize}
\item {} 
User network : Stores the user information and connections between the user nodes

\item {} 
Tweet network : Stores the tweets, their attributes and interconnections between tweets and their connections with user nodes as well.

\item {} 
Indexing network : Stores the time indexing structure utilized to answer queries having a temporal dimension. Its instructive to imagine the user and tweet network on a plane and the indexing network on top of this plane.

\end{itemize}

Let’s look at these networks in some detail


\subsection{User network}
\label{\detokenize{neo4j_data_ingestion:user-network}}
The user network contains following nodes:
\begin{itemize}
\item {} 
USER node : It contains user\_id and screen\_name.

\item {} 
USER\_INFO node: It contains the user info like number of followers/friends, number of tweets, location and other meta data. These nodes form a link list in which new nodes are added, when information of the user is collected.

\end{itemize}

\noindent\sphinxincludegraphics{{neo_in3}.png}

\noindent\sphinxincludegraphics{{neo_in2}.png}


\subsection{Tweet network}
\label{\detokenize{neo4j_data_ingestion:tweet-network}}
The tweet network contains following nodes:
\begin{itemize}
\item {} 
TWEET node : It contains the tweet info like its id, the raw text of the tweet, the location from which the tweet is made(currently we are storing it in raw fashion, which in future can be extended by creating LOCATION nodes just like HASHTAG et al.)

\item {} 
HASHTAG node : It contains the text of the hashtag.

\item {} 
URL node : Contains the full and the shortened url.

\end{itemize}

\noindent\sphinxincludegraphics{{neo_in1}.png}


\subsection{Indexing network}
\label{\detokenize{neo4j_data_ingestion:indexing-network}}
The Indexing network contains:
\begin{itemize}
\item {} 
RUN node : It is a unique node in the entire network. Just a starting placeholder for the root of the indexing structure.

\item {} 
FRAME node : A frame time interval is to be specified by the user. Each frame indexes time slices equal to the specified interval.

\item {} 
FOLLOW\_EVENT node : represents that Follower started following Followee.

\item {} 
UNFOLLOW\_EVENT node : represents that Unfollower stopped following Unfollowee.

\item {} 
TWEET\_EVENT node : represents that user tweeted tweet.

\end{itemize}

\noindent\sphinxincludegraphics{{neo_in4}.png}


\section{Ingesting the data into neo4j : Logic}
\label{\detokenize{neo4j_data_ingestion:ingesting-the-data-into-neo4j-logic}}
We get a tweet from the twitter firehose. One simple thing could be to make a connection to the on-disk database and insert the tweet. This can be achieved using \sphinxtitleref{session.run(\textless{}cypher tweet insertion query\textgreater{})}, because run in neo4j emulates a auto-commit transaction.


\subsection{Improving ingestion rate using transaction}
\label{\detokenize{neo4j_data_ingestion:improving-ingestion-rate-using-transaction}}
A simple way to make this more efficient would be the use of transactions. The idea behind using transactions is to keep on accumulating the incoming tweets in a graph in memory. After some fixed number of tweets, the transaction is committed to the disk. Clearly this leads to faster ingestion rate:
\begin{itemize}
\item {} 
Accumulating to memory and then writing the batch to disk is faster as compared to writing tweet by tweet to disk due to the efficiency in disk head seaks.

\item {} 
Further, when creating the in-memory local graph from the transaction batch, neo4j does some changes in the order in which to write the changes to ensure efficiency.

\end{itemize}

But, the clear downside of this is that the queries being answered will lag behind at most the transaction size. This happens because the tweets are being inserted in real time manner and the queries are also being answered simultaneously. But this is not a major issue as it induces a lag of only \textless{}10 secs(assuming transaction size of \textasciitilde{}30k).


\subsection{Indexing}
\label{\detokenize{neo4j_data_ingestion:indexing}}
We create uniqueness constraints on the following attributes of these nodes:


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline
\sphinxstylethead{\sphinxstyletheadfamily 
Node
\unskip}\relax &\sphinxstylethead{\sphinxstyletheadfamily 
Attribute
\unskip}\relax \\
\hline
FRAME
&
start\_t
\\
\hline
TWEET
&
id
\\
\hline
USER
&
id
\\
\hline
HASHTAG
&
text
\\
\hline
URL
&
url
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\section{Running the ingestion script}
\label{\detokenize{neo4j_data_ingestion:running-the-ingestion-script}}
There are 2 applications to ingest data into Neo4j.


\subsection{Streaming data}
\label{\detokenize{neo4j_data_ingestion:streaming-data}}
If the data has been collected using the Stream Sample API (as explained in {\hyperref[\detokenize{twitter_stream:stream-sample-api}]{\sphinxcrossref{\DUrole{std,std-ref}{Stream Sample API}}}}), then this application needs to be used to ingest the tweets into Neo4j. Navigate to the Ingestion/Neo4j and make changes to the file ingest\_neo4j\_streaming.py. Specifically, provide the folder containing the tweets containing files. We are simulating the twitter stream by reading the tweets from a file on the disk and storing those in memory. This makes sense as we can’t possibly get tweets from the twitter hose at a rate greater than reading from memory, thus this in no way can be a bottleneck to the ingestion rate. Then just run the file \sphinxtitleref{python ingest\_neo4j\_streaming.py} to start ingesting. A logs file will be created which will keep on updating to help the user gauze the ingestion rate.


\subsection{User Timeline data}
\label{\detokenize{neo4j_data_ingestion:user-timeline-data}}
If the data has been collected using the User Timeline API (as explained in {\hyperref[\detokenize{twitter_stream:user-timeline-api}]{\sphinxcrossref{\DUrole{std,std-ref}{User Timeline API}}}}), then this application needs to be used to ingest the tweets into Neo4j. Navigate to the Ingestion/Neo4j and make changes to the file ingest\_neo4j\_user\_timeline.py. This file ingests all the data present in the ‘data’ folder in the same directory. However, if the data is collected in an incremental fashion (eg. running the data collection application day by day), then this file needs to be changed accordingly to only ingest the data of the new timestamps.


\section{Neo4j Ingestion Rates}
\label{\detokenize{neo4j_data_ingestion:neo4j-ingestion-rates}}
\noindent\sphinxincludegraphics{{neo_in5}.png}

Observe that the ingestion rate peaks at 1000 tweets/sec at a transaction size of around 35k. This is mainly due to the limitation of memory size. The authors observe that keeping a larger transaction size leads to lag on the system indicative of use of swap space. Thus, the maximum ingestion rate can be enhanced just by putting in more memory, albeit with decreasing returns.


\section{Code Documentation for Neo4j data ingestion}
\label{\detokenize{neo4j_data_ingestion:code-documentation-for-neo4j-data-ingestion}}
Here we provide a documentation of the code for ingesting tweets collected using Streaming API.
\phantomsection\label{\detokenize{neo4j_data_ingestion:module-ingest_neo4j_streaming}}\index{ingest\_neo4j\_streaming (module)}
Module to insert data, collected using streaming API, into Neo4j database.

The {\hyperref[\detokenize{neo4j_data_ingestion:module-ingest_neo4j_streaming}]{\sphinxcrossref{\sphinxcode{ingest\_neo4j\_streaming}}}} module contains the classes:
\begin{itemize}
\item {} 
{\hyperref[\detokenize{neo4j_data_ingestion:ingest_neo4j_streaming.Twitter}]{\sphinxcrossref{\sphinxcode{ingest\_neo4j\_streaming.Twitter}}}}

\end{itemize}

One can use the \sphinxcode{ingest\_neo4j\_streaming.Twitter.ingest\_tweet()} to insert a new tweet into the database.

An example usage where we want to insert all tweets from all files in a folder tweet\_folder:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{t} \PYG{o}{=} \PYG{n}{Twitter}\PYG{p}{(}\PYG{l+m+mi}{50000}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{t}\PYG{o}{.}\PYG{n}{get\PYGZus{}constraints}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{t}\PYG{o}{.}\PYG{n}{get\PYGZus{}profile}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{read\PYGZus{}tweets}\PYG{p}{(}\PYG{o}{\PYGZlt{}}\PYG{n}{tweet\PYGZus{}folder}\PYG{o}{\PYGZgt{}}\PYG{p}{,} \PYG{n}{t}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{t}\PYG{o}{.}\PYG{n}{get\PYGZus{}profile}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{t}\PYG{o}{.}\PYG{n}{close\PYGZus{}session}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}
\index{Twitter (class in ingest\_neo4j\_streaming)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_streaming.Twitter}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{ingest\_neo4j\_streaming.}\sphinxbfcode{Twitter}}{\emph{batch\_size=200}}{}
Bases: \sphinxcode{object}

Class containing functions to insert tweets into neo4j.
We open a connection to the neo4j database through py2neo and the official
neo4j driver. Dealing with transactions is easy in py2neo, so it is used to make and commit transactions. While
the connection to the neo4j driver is used just in clearing out the graph.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{batch\_size} \textendash{} number of tweets to take into transaction before commiting it

\end{description}\end{quote}
\index{clear\_graph() (ingest\_neo4j\_streaming.Twitter method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_streaming.Twitter.clear_graph}}\pysiglinewithargsret{\sphinxbfcode{clear\_graph}}{}{}
Delete the complete graph. Albiet, keep the indices.

\end{fulllineitems}

\index{close() (ingest\_neo4j\_streaming.Twitter method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_streaming.Twitter.close}}\pysiglinewithargsret{\sphinxbfcode{close}}{}{}
See if there are some tweets not comitted in the trnx and if yes, commit those.

\end{fulllineitems}

\index{close\_session() (ingest\_neo4j\_streaming.Twitter method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_streaming.Twitter.close_session}}\pysiglinewithargsret{\sphinxbfcode{close\_session}}{}{}
Close the neo4j driver session

\end{fulllineitems}

\index{create\_constraint() (ingest\_neo4j\_streaming.Twitter method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_streaming.Twitter.create_constraint}}\pysiglinewithargsret{\sphinxbfcode{create\_constraint}}{\emph{node}, \emph{attrib}}{}
Create constraint on attrib in node node
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{node} \textendash{} node type on which to create the constraint

\item[{Parem attrib}] \leavevmode
node attibute whose constraint is to be created

\end{description}\end{quote}

\end{fulllineitems}

\index{create\_constraints() (ingest\_neo4j\_streaming.Twitter method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_streaming.Twitter.create_constraints}}\pysiglinewithargsret{\sphinxbfcode{create\_constraints}}{}{}
Create uniqueness constraints on the attributes of nodes. Note that ceating a constraint automatically
creates an index on it

\end{fulllineitems}

\index{drop\_constraint() (ingest\_neo4j\_streaming.Twitter method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_streaming.Twitter.drop_constraint}}\pysiglinewithargsret{\sphinxbfcode{drop\_constraint}}{\emph{node}, \emph{attrib}}{}
Drop constraint on attrib in node node
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{node} \textendash{} node type on which to delete the constraint

\item[{Parem attrib}] \leavevmode
node attibute whose constraint is to be deleted

\end{description}\end{quote}

\end{fulllineitems}

\index{get\_constraints() (ingest\_neo4j\_streaming.Twitter method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_streaming.Twitter.get_constraints}}\pysiglinewithargsret{\sphinxbfcode{get\_constraints}}{}{}
Get the constrainsts on different types of nodes.

\end{fulllineitems}

\index{get\_profile() (ingest\_neo4j\_streaming.Twitter method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_streaming.Twitter.get_profile}}\pysiglinewithargsret{\sphinxbfcode{get\_profile}}{}{}
The number of total, user, tweet, hashtag nodes in the graph

\end{fulllineitems}

\index{insert\_tweet() (ingest\_neo4j\_streaming.Twitter method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_streaming.Twitter.insert_tweet}}\pysiglinewithargsret{\sphinxbfcode{insert\_tweet}}{\emph{tweet}, \emph{favourited\_by=None}, \emph{fav\_timestamp=None}}{}
The main function to insert the tweet. Begin a transaction, when atleast batch\_size number of tweets
are collected, commit the transaction. Start collecting the new tweets after that.

We have two cases depending if the tweet to be inserted is a
retweet or not. We mention the steps taken to insert the tweet in the two case:
\begin{itemize}
\item {} \begin{description}
\item[{The tweet is a retweet}] \leavevmode\begin{itemize}
\item {} 
Create a tweet\_event under a appropriate frame

\item {} 
Merge node for this tweet. Maybe the tweet node already partially exists because some other tweet is its reply.

\item {} 
Create favorite relation if needed

\item {} 
Proceed only if the tweet was not already created

\item {} 
Create user and then the relationships

\item {} 
Find node of original tweet and link

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{The tweet is not a retweet}] \leavevmode\begin{itemize}
\item {} 
Create a tweet\_event under a appropriate frame

\item {} 
Merge node for this tweet

\item {} 
Create favorite relation if needed

\item {} 
Proceed only if the tweet was not already created

\item {} 
Create user and then the relationships

\item {} 
Create links to hashtags, mentions, urls

\item {} 
Create link to quoted tweet in case this tweet quotes another tweet

\item {} 
Create link to original tweet in case this is a reply tweet

\end{itemize}

\end{description}

\end{itemize}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{tweet} \textendash{} the json of the tweet to be inserted

\item {} 
\sphinxstyleliteralstrong{favourited\_by} \textendash{} userid of the user who favourited the tweet

\item {} 
\sphinxstyleliteralstrong{fav\_timestamp} \textendash{} time at which the tweet was favourited

\end{itemize}

\item[{Returns}] \leavevmode
None

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Todo:}
Currently we are making use of transactions only. We get decent peak ingestion rate of
around 1000 tweets/sec. But this can be increased by overlapping the collection and ingestion part as
we do in case of mongoDB. But the same scheme can’t be used here as the transaction created is not
a native python object and hance can’t be passed between python multiprocessing module processes. So, one
idea is to create a csv with the tweets, and then call the use\_csv function in neo4j to ingest. But this is a
contrived way of doing this by doing same task twice.
\end{sphinxadmonition}

\end{fulllineitems}


\end{fulllineitems}

\index{flatten\_json() (in module ingest\_neo4j\_streaming)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_streaming.flatten_json}}\pysiglinewithargsret{\sphinxcode{ingest\_neo4j\_streaming.}\sphinxbfcode{flatten\_json}}{\emph{json\_obj}}{}
Function to flatten the tweet. Used in case we want to store the complete tweet JSON in the TWEET node.
This is because neo4j doesn’t allow nested jsons to be stored

\end{fulllineitems}

\index{getDateFromTimestamp() (in module ingest\_neo4j\_streaming)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_streaming.getDateFromTimestamp}}\pysiglinewithargsret{\sphinxcode{ingest\_neo4j\_streaming.}\sphinxbfcode{getDateFromTimestamp}}{\emph{timestamp}}{}
\end{fulllineitems}

\index{getFrameStartEndTime() (in module ingest\_neo4j\_streaming)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_streaming.getFrameStartEndTime}}\pysiglinewithargsret{\sphinxcode{ingest\_neo4j\_streaming.}\sphinxbfcode{getFrameStartEndTime}}{\emph{timestamp}}{}
\end{fulllineitems}

\index{log() (in module ingest\_neo4j\_streaming)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_streaming.log}}\pysiglinewithargsret{\sphinxcode{ingest\_neo4j\_streaming.}\sphinxbfcode{log}}{\emph{text}}{}
Why use this when we can use logging? There is a peculiar bug when open neo4j bolt server with logging

\end{fulllineitems}

\index{read\_tweets() (in module ingest\_neo4j\_streaming)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_streaming.read_tweets}}\pysiglinewithargsret{\sphinxcode{ingest\_neo4j\_streaming.}\sphinxbfcode{read\_tweets}}{\emph{path}, \emph{twitter}, \emph{filename=''}}{}
Read tweets from the directory in path and inert all tweets in all files in the first level of path into
neo4j.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{path} \textendash{} the path of the directory

\item {} 
\sphinxstyleliteralstrong{twitter} \textendash{} a Twitter object

\item {} 
\sphinxstyleliteralstrong{filename} \textendash{} optional, if want to insert tweets from a single file

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


Here we provide a documentation of the code for ingesting data collected using User Timeline API.
\phantomsection\label{\detokenize{neo4j_data_ingestion:module-ingest_neo4j_user_timeline}}\index{ingest\_neo4j\_user\_timeline (module)}
Module to insert data, collected using user timeline API, into Neo4j database.

This is the schema of the Neo4j graph:
\begin{description}
\item[{USER NETWORK}] \leavevmode
Node labels - USER(id), USER\_INFO(dict)
Relationships - CURR\_STATE(from), INTITIAL\_STATE(on), PREV(from,to)

\item[{FOLLOWER NETWORK}] \leavevmode
Node labels -
Relationships - FOLLOWS(from,to), FOLLOWED(from,to) // FOLLOWS.to will always be the last time data was collected

\item[{TWEET NETWORK}] \leavevmode
Node labels - TWEET(id,created\_at,is\_active), TWEET\_INFO(dict), HASHTAG(text), URL(url,expanded\_url), //MEDIA(url, media\_url)//, PLACE(id,name,country) -\textgreater{} This is not the location of tweet but the location with which the tweet is tagged (could be about it)
Relationships - TWEETED(on), LIKES(on), INFO, REPLY\_TO(on), RETWEET\_OF(on), QUOTED(on), HAS\_MENTION(on), HAS\_HASHTAG(on), //HAS\_MEDIA(on)//, HAS\_URL(on), HAS\_PLACE(on)

\item[{FRAME NETWORK}] \leavevmode
Node labels - RUN, FRAME(start\_t,end\_t), TWEET\_EVENT(timestamp), FOLLOW\_EVENT(timestamp), UNFOLLOW\_EVENT(timestamp), FAV\_EVENT(timestamp)
Relationships - HAS\_FRAME, HAS\_TWEET, TE\_USER, TE\_TWEET, HAS\_FOLLOW, FE\_FOLLOWED, FE\_FOLLOWS, HAS\_UNFOLLOW, UFE\_UNFOLLOWED, UFE\_UNFOLLOWS, HAS\_FAV, FAV\_USER, FAV\_TWEET

\end{description}
\index{Neo4jIngestion\_UserTimeline (class in ingest\_neo4j\_user\_timeline)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_user_timeline.Neo4jIngestion_UserTimeline}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{ingest\_neo4j\_user\_timeline.}\sphinxbfcode{Neo4jIngestion\_UserTimeline}}{\emph{data\_directory}, \emph{user\_screen\_names\_file\_path}}{}
Bases: \sphinxcode{object}
\index{clear\_db() (ingest\_neo4j\_user\_timeline.Neo4jIngestion\_UserTimeline method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_user_timeline.Neo4jIngestion_UserTimeline.clear_db}}\pysiglinewithargsret{\sphinxbfcode{clear\_db}}{}{}
\end{fulllineitems}

\index{close\_session() (ingest\_neo4j\_user\_timeline.Neo4jIngestion\_UserTimeline method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_user_timeline.Neo4jIngestion_UserTimeline.close_session}}\pysiglinewithargsret{\sphinxbfcode{close\_session}}{}{}
\end{fulllineitems}

\index{create\_indexes() (ingest\_neo4j\_user\_timeline.Neo4jIngestion\_UserTimeline method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_user_timeline.Neo4jIngestion_UserTimeline.create_indexes}}\pysiglinewithargsret{\sphinxbfcode{create\_indexes}}{}{}
\end{fulllineitems}

\index{create\_tweet() (ingest\_neo4j\_user\_timeline.Neo4jIngestion\_UserTimeline method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_user_timeline.Neo4jIngestion_UserTimeline.create_tweet}}\pysiglinewithargsret{\sphinxbfcode{create\_tweet}}{\emph{tweet}, \emph{favourited\_by=None}, \emph{fav\_timestamp=None}}{}
\end{fulllineitems}

\index{flatten\_json() (ingest\_neo4j\_user\_timeline.Neo4jIngestion\_UserTimeline method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_user_timeline.Neo4jIngestion_UserTimeline.flatten_json}}\pysiglinewithargsret{\sphinxbfcode{flatten\_json}}{\emph{json\_obj}}{}
\end{fulllineitems}

\index{getFrameStartEndTime() (ingest\_neo4j\_user\_timeline.Neo4jIngestion\_UserTimeline method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_user_timeline.Neo4jIngestion_UserTimeline.getFrameStartEndTime}}\pysiglinewithargsret{\sphinxbfcode{getFrameStartEndTime}}{\emph{timestamp}}{}
\end{fulllineitems}

\index{readDataAndCreateGraph() (ingest\_neo4j\_user\_timeline.Neo4jIngestion\_UserTimeline method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_user_timeline.Neo4jIngestion_UserTimeline.readDataAndCreateGraph}}\pysiglinewithargsret{\sphinxbfcode{readDataAndCreateGraph}}{\emph{STAGE\_BY\_STAGE}}{}
\end{fulllineitems}

\index{simulateExample() (ingest\_neo4j\_user\_timeline.Neo4jIngestion\_UserTimeline method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_user_timeline.Neo4jIngestion_UserTimeline.simulateExample}}\pysiglinewithargsret{\sphinxbfcode{simulateExample}}{}{}
\end{fulllineitems}

\index{sync\_session() (ingest\_neo4j\_user\_timeline.Neo4jIngestion\_UserTimeline method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_user_timeline.Neo4jIngestion_UserTimeline.sync_session}}\pysiglinewithargsret{\sphinxbfcode{sync\_session}}{\emph{type\_=None}}{}
\end{fulllineitems}

\index{update\_followers() (ingest\_neo4j\_user\_timeline.Neo4jIngestion\_UserTimeline method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_user_timeline.Neo4jIngestion_UserTimeline.update_followers}}\pysiglinewithargsret{\sphinxbfcode{update\_followers}}{\emph{user\_id}, \emph{follower\_ids}, \emph{timestamp}}{}
\end{fulllineitems}

\index{update\_friends() (ingest\_neo4j\_user\_timeline.Neo4jIngestion\_UserTimeline method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_user_timeline.Neo4jIngestion_UserTimeline.update_friends}}\pysiglinewithargsret{\sphinxbfcode{update\_friends}}{\emph{user\_id}, \emph{friend\_ids}, \emph{timestamp}}{}
\end{fulllineitems}

\index{update\_user() (ingest\_neo4j\_user\_timeline.Neo4jIngestion\_UserTimeline method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_user_timeline.Neo4jIngestion_UserTimeline.update_user}}\pysiglinewithargsret{\sphinxbfcode{update\_user}}{\emph{id}, \emph{user\_info\_dict}, \emph{timestamp}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{log() (in module ingest\_neo4j\_user\_timeline)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_data_ingestion:ingest_neo4j_user_timeline.log}}\pysiglinewithargsret{\sphinxcode{ingest\_neo4j\_user\_timeline.}\sphinxbfcode{log}}{\emph{text}}{}
\end{fulllineitems}



\chapter{Ingesting data into MongoDB}
\label{\detokenize{mongoDB_data_ingestion:ingesting-data-into-mongodb}}\label{\detokenize{mongoDB_data_ingestion::doc}}

\section{Why store in MongoDB}
\label{\detokenize{mongoDB_data_ingestion:why-store-in-mongodb}}
In mongoDB we store only the data which can be extracted quickly from incoming tweets without much processing. We answer “non-network” based i.e. simple aggregate queries using MongoDB instead of Neo4j.

This means that any query which can be answered using mongoDB can also be answered using the network data in neo4j. This has been done to ensure that some very common queries can be answered quickly. Also, neo4j has a limit on the parallel sessions that can be made to the database, so in case we decide to do away with mongoDB, those queries would have to be answered from neo4j and would unnecessarily take up the sessions.


\section{Data Format in mongoDB}
\label{\detokenize{mongoDB_data_ingestion:data-format-in-mongodb}}
We have three collections in mongoDB:
\begin{itemize}
\item {} \begin{description}
\item[{To store the hashtags. Each document in this collection stores the following information:}] \leavevmode\begin{itemize}
\item {} 
the hashtag

\item {} 
the timestamp of the tweet which contained the hashtag

\item {} 
the sentiment associated with the tweet containing the hashtag

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{To store urls}] \leavevmode\begin{itemize}
\item {} 
the url

\item {} 
the timestamp of the tweet which contained the hashtag

\item {} 
the sentiment associated with the tweet containing the hashtag

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{To store user mentions}] \leavevmode\begin{itemize}
\item {} 
the user mention

\item {} 
the timestamp of the tweet which contained the hashtag

\item {} 
the sentiment associated with the tweet containing the hashtag

\end{itemize}

\end{description}

\end{itemize}

Given this information in mongoDB, we can currently use it to answer queries like:
\begin{itemize}
\item {} 
Most popular hashtags(and their sentiment) in total

\item {} 
Most popular hashtags(and their sentiment) in an interval of time

\item {} 
Most popular urls in total

\item {} 
Most popular urls in an interval of time

\item {} 
Most popular users in total(in terms of their mentions)

\item {} 
Most popular users in an interval of time(in terms of their mentions)

\end{itemize}


\section{mongoDB v/s neo4j}
\label{\detokenize{mongoDB_data_ingestion:mongodb-v-s-neo4j}}
Note that just the bare minimum information that is currently being stored in the mongoDB. It can easily be extended to store more information. MongoDB provides strong mechanisms to aggregate and extract information
from the database.

So, even if we decide to store some pseudo-structural information, like the user of the tweet in hashtags collection and then answer queries like the sentiment associated will all the tweets of an user, we expect the query execution time to be atleast as fast as answering the query in neo4j, though in case of neo4j also, answering such query would also take only a single hop, which means that the execution time would be small anyways. This is precisely the reason why we don’t currently store such information in mongoDB.

But, as the size of the system grows, it would surely be beneficial to store much more condensed data in mongoDB and use it to answer more complex queries.


\section{Ingesting the data into mongoDB : Logic}
\label{\detokenize{mongoDB_data_ingestion:ingesting-the-data-into-mongodb-logic}}
{[}scheme 1{]} A simple approach would be to ingest a tweet into the database as and when it comes in real time. But clearly(and as mentioned in mongoDB documentation) this is suboptimal, as we are connecting to the on-disk database frequently.


\subsection{Improving ingestion rate using transactions}
\label{\detokenize{mongoDB_data_ingestion:improving-ingestion-rate-using-transactions}}
{[}scheme 2{]} An easy solution to this would be to keep collecting the data in memory and then write it to the database periodically in batches.

But observe that, the time it takes the process to open a connection to database and then write the data to it, no new tweets are being collected in memory.


\subsection{Improving ingestion rate using parallel multiple process}
\label{\detokenize{mongoDB_data_ingestion:improving-ingestion-rate-using-parallel-multiple-process}}
{[}scheme 3{]} So finally we take the approach of utilizing multiple processes to write data to mongoDB.

Observe here the distinction between a thread and a process.
While using multiple threads, the threads are run(usually, if we discount the kernel threads spawned by python) on a single core in python, due to Global Interpreter Lock and thus, though we get virtual parallelism,
we don’t get real parallelism. Thus, due to the limitation of the language, we are using process to get the parallelism between writing to database and collecting new tweets.
A clear disadvantage of using process over threads will become clear below.

To explain the final multi-process approach, we have two processes running:
\begin{itemize}
\item {} 
Accumulator process - It collects the tweets in an in-memory data structure. Also, in the beginning at t=0, it spawns a timer thread, which generates an interrupt after every pre-specified T time.

\item {} 
Connector process - It takes a list of tweets through a pipe, opens connection to the database and writes the tweets to the database.

\end{itemize}

How the system works can be understood through this image:

\noindent\sphinxincludegraphics{{mongo_ingestion}.jpg}

So, the timer process in the accumulator process generates an interrupt after every T seconds, at this instant, the accumulator stops collecting tweets and writes those to Inter process communication(IPC) pipe. This is generally fast as IPC pipe are implemented in memory. Now, the other end of the pipe is in the connector process. After the writing process has been complete, it receives the tweets and starts writing those to the on-disk database as a batch, which again ensures that the process is faster as compared to writing single tweet at a time in a loop. Concurrently, while the connector process is writing the tweets, the accumulator process starts accumulating new tweets.

So in this way the the process of writing to database in connector process is overlapped with the the accumulation of tweets in accumulator process. Note that we have a small gap equivalent to time taken to write to IPC, in which the accumulator process is not collecting the tweets. The whole process can further be made efficient by removing this gap, but since we are getting tweet ingestion rate much more than the rate of tweets coming on twitter and the gain from removing the gap would not be much, we don’t implement it.

To answer queries like the most popular hashtags in total, or most popular hashtags in a large interval. It would be beneficial to have aggregates over a larger interval. For example, say we want to get the most popular hashtags in an year, it would be helpful in that setting to have an aggregated document containing 100 most popular hashtags in each month, then we can consider a union of these 12 documents plus some counting from the interval edges to get the most popular hashtags. Clearly, this will fasten the query answering rate. Though, this would not always give the exactly accurate results and can also not be used to get the counts of hashtags, but can be used to get most popular k hashtags as the size of data grows. To implement it, simply spawn another thread in the connector process to read data from the hashtags collection at a specific time interval(like 1 week), aggregate the data and store the aggregated information into a new collection. We provide the code for this, but don’t currently use this mechanism.


\section{Ingesting the data into mongoDB : Practical side}
\label{\detokenize{mongoDB_data_ingestion:ingesting-the-data-into-mongodb-practical-side}}
On practical side, to ingest data into mongoDB, navigate to the Ingestion/MonogDB and make changes to the file ingest\_raw.py. Specifically, provide the folder containing the tweets containing files. We are simulating the twitter stream by reading the tweets from a file on the disk and storing those in memory. This makes sense as we can’t possibly get tweets from the twitter hose at a rate greater than reading from memory, thus this in no way can be a bottleneck to the ingestion rate. Then just run the we need to run the file \sphinxtitleref{python ingest\_raw.py} to start ingesting. A logs file will be created which will keep on updating to help the user gauze the ingestion rate.

Please observe that the process of ingesting into neo4j and mongoDB are similar, with just variations in which code to run.


\section{MongoDB Ingestion Rates}
\label{\detokenize{mongoDB_data_ingestion:mongodb-ingestion-rates}}
As expected, the ingestion rate into mongoDB while overlapping writing into database and accumulating data is faster than without parallelization. The plot below shows a comparison between scheme 2 and scheme 3 as described above. Observe that as more and more tweets are inserted, the difference between the two scheme grows as the time saved in overlapping inserting the accumulating keeps on adding up in advantage of scheme 3.

\noindent\sphinxincludegraphics{{image1}.png}

Clearly the ingestion rate depends on the time after which the interrupt to start write the collected tweets to database is generate(called T in {\hyperref[\detokenize{mongoDB_data_ingestion:improving-ingestion-rate-using-parallel-multiple-process}]{\sphinxcrossref{\DUrole{std,std-ref}{Improving ingestion rate using parallel multiple process}}}}).

Finally we get an ingestion rate of around 7k-12k(around x10 of that of neo4j) tweets/second on average, depending on T.


\section{Code Documentation for mongoDB ingestion}
\label{\detokenize{mongoDB_data_ingestion:module-ingest_raw}}\label{\detokenize{mongoDB_data_ingestion:code-documentation-for-mongodb-ingestion}}\index{ingest\_raw (module)}
Module to ingest data into MongoDB. We try to overlay the collection and ingestion of tweets while ingeting data. For more details, see the documenation.

The {\hyperref[\detokenize{mongoDB_data_ingestion:module-ingest_raw}]{\sphinxcrossref{\sphinxcode{ingest\_raw}}}} module contains the classes:
\begin{itemize}
\item {} 
{\hyperref[\detokenize{mongoDB_data_ingestion:ingest_raw.Ingest}]{\sphinxcrossref{\sphinxcode{ingest\_raw.Ingest}}}}

\end{itemize}

One can use the {\hyperref[\detokenize{mongoDB_data_ingestion:ingest_raw.Ingest.insert_tweet}]{\sphinxcrossref{\sphinxcode{ingest\_raw.Ingest.insert\_tweet()}}}} to insert a new tweet into the database.

An example usage where we want to insert all tweets from all files in a folder tweet\_folder:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{i} \PYG{o}{=} \PYG{n}{Ingest}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{i}\PYG{o}{.}\PYG{n}{clear\PYGZus{}db}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{read\PYGZus{}tweets}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{o}{\PYGZlt{}}\PYG{n}{tweet\PYGZus{}folder}\PYG{o}{\PYGZgt{}}\PYG{p}{)}
\end{sphinxVerbatim}
\index{Ingest (class in ingest\_raw)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Ingest}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{ingest\_raw.}\sphinxbfcode{Ingest}}{\emph{interval}}{}
Bases: \sphinxcode{object}

Class to insert tweets into mongoDB.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{interval} \textendash{} time interval after which to generate interupt for starting ingestion of tweets

\end{description}\end{quote}
\index{aggregate() (ingest\_raw.Ingest method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Ingest.aggregate}}\pysiglinewithargsret{\sphinxbfcode{aggregate}}{}{}
The function to be called in case, we choose to aggregate the counts at a larger inteeval.

\begin{sphinxadmonition}{note}{Note:}
interval1\textgreater{}\textgreater{}interval. interval is like order of seconds and interal1 is like order of hours.
\end{sphinxadmonition}

\end{fulllineitems}

\index{clear\_db() (ingest\_raw.Ingest method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Ingest.clear_db}}\pysiglinewithargsret{\sphinxbfcode{clear\_db}}{}{}
Delete all the collections

\end{fulllineitems}

\index{exit() (ingest\_raw.Ingest method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Ingest.exit}}\pysiglinewithargsret{\sphinxbfcode{exit}}{}{}
Join the worker process

\end{fulllineitems}

\index{insert\_tweet() (ingest\_raw.Ingest method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Ingest.insert_tweet}}\pysiglinewithargsret{\sphinxbfcode{insert\_tweet}}{\emph{tweet}}{}
Function to collect incoming real time tweets. Update the in memory dictionaries.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{tweet} \textendash{} the json of the tweet.

\end{description}\end{quote}

..note:: If we choose to keep new information about the tweets, we need to modify this, along with {\hyperref[\detokenize{mongoDB_data_ingestion:ingest_raw.Ingest.worker}]{\sphinxcrossref{\sphinxcode{ingest\_raw.Ingest.worker()}}}}.

\end{fulllineitems}

\index{populate() (ingest\_raw.Ingest method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Ingest.populate}}\pysiglinewithargsret{\sphinxbfcode{populate}}{}{}
The code executed by the collector process. After each interupt it spawns a new timer thread to generate the
new interupt. Also, it puts the collected tweets into the IPC pipe and starts collecting new tweets.

\end{fulllineitems}

\index{worker() (ingest\_raw.Ingest method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Ingest.worker}}\pysiglinewithargsret{\sphinxbfcode{worker}}{\emph{q}}{}
The function called inside he ingestor(worker) process. This function is alled after every \textless{}interval\textgreater{}
seconds. It loops to look for inputs from the pipe end. Once inputs are there, it opens connection to
database and commits the batch recieved from the pipe to the on-disk database.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{q} \textendash{} the inter-process communication pipe

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{Timer (class in ingest\_raw)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{ingest\_raw.}\sphinxbfcode{Timer}}{\emph{interval}, \emph{function}, \emph{args=None}, \emph{kwargs=None}, \emph{iterations=1}, \emph{infinite=False}}{}
Bases: \sphinxcode{multiprocessing.context.Process}

Calls a function after a specified number of seconds:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{t} \PYG{o}{=} \PYG{n}{Timer}\PYG{p}{(}\PYG{l+m+mf}{30.0}\PYG{p}{,} \PYG{n}{f}\PYG{p}{,} \PYG{n}{args}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{kwargs}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{t}\PYG{o}{.}\PYG{n}{start}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{t}\PYG{o}{.}\PYG{n}{cancel}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{}stops the timer if it is still waiting}
\end{sphinxVerbatim}
\index{authkey (ingest\_raw.Timer attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.authkey}}\pysigline{\sphinxbfcode{authkey}}
\end{fulllineitems}

\index{cancel() (ingest\_raw.Timer method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.cancel}}\pysiglinewithargsret{\sphinxbfcode{cancel}}{}{}
Stop the timer if it hasn’t already finished.

\end{fulllineitems}

\index{daemon (ingest\_raw.Timer attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.daemon}}\pysigline{\sphinxbfcode{daemon}}
Return whether process is a daemon

\end{fulllineitems}

\index{exitcode (ingest\_raw.Timer attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.exitcode}}\pysigline{\sphinxbfcode{exitcode}}
Return exit code of process or \sphinxtitleref{None} if it has yet to stop

\end{fulllineitems}

\index{ident (ingest\_raw.Timer attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.ident}}\pysigline{\sphinxbfcode{ident}}
Return identifier (PID) of process or \sphinxtitleref{None} if it has yet to start

\end{fulllineitems}

\index{is\_alive() (ingest\_raw.Timer method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.is_alive}}\pysiglinewithargsret{\sphinxbfcode{is\_alive}}{}{}
Return whether process is alive

\end{fulllineitems}

\index{join() (ingest\_raw.Timer method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.join}}\pysiglinewithargsret{\sphinxbfcode{join}}{\emph{timeout=None}}{}
Wait until child process terminates

\end{fulllineitems}

\index{name (ingest\_raw.Timer attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.name}}\pysigline{\sphinxbfcode{name}}
\end{fulllineitems}

\index{pid (ingest\_raw.Timer attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.pid}}\pysigline{\sphinxbfcode{pid}}
Return identifier (PID) of process or \sphinxtitleref{None} if it has yet to start

\end{fulllineitems}

\index{run() (ingest\_raw.Timer method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.run}}\pysiglinewithargsret{\sphinxbfcode{run}}{}{}
\end{fulllineitems}

\index{sentinel (ingest\_raw.Timer attribute)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.sentinel}}\pysigline{\sphinxbfcode{sentinel}}
Return a file descriptor (Unix) or handle (Windows) suitable for
waiting for process termination.

\end{fulllineitems}

\index{start() (ingest\_raw.Timer method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.start}}\pysiglinewithargsret{\sphinxbfcode{start}}{}{}
Start child process

\end{fulllineitems}

\index{terminate() (ingest\_raw.Timer method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.Timer.terminate}}\pysiglinewithargsret{\sphinxbfcode{terminate}}{}{}
Terminate process; sends SIGTERM signal or uses TerminateProcess()

\end{fulllineitems}


\end{fulllineitems}

\index{calculate\_sentiment (in module ingest\_raw)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.calculate_sentiment}}\pysigline{\sphinxcode{ingest\_raw.}\sphinxbfcode{calculate\_sentiment}}
Function to calculate sentiment of a tweet. Just calculate the number of positive and negative words,
matching against a pre-curated list.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{positive\_words} \textendash{} the list of positive sentiment words

\item {} 
\sphinxstyleliteralstrong{negative\_words} \textendash{} the list of negative sentiment words

\item {} 
\sphinxstyleliteralstrong{tweet\_text} \textendash{} the raw test of the tweet splitted tokenized, hashtags and user-mentions removed

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{getDateFromTimestamp() (in module ingest\_raw)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.getDateFromTimestamp}}\pysiglinewithargsret{\sphinxcode{ingest\_raw.}\sphinxbfcode{getDateFromTimestamp}}{\emph{timestamp}}{}
\end{fulllineitems}

\index{read\_tweets() (in module ingest\_raw)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.read_tweets}}\pysiglinewithargsret{\sphinxcode{ingest\_raw.}\sphinxbfcode{read\_tweets}}{\emph{ingest}, \emph{path}, \emph{filename=''}}{}
Read tweets from the directory in path and inert all tweets in all files in the first level of path into
neo4j.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{path} \textendash{} the path of the directory

\item {} 
\sphinxstyleliteralstrong{twitter} \textendash{} a Twitter object

\item {} 
\sphinxstyleliteralstrong{filename} \textendash{} optional, if want to insert tweets from a single file

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{threaded() (in module ingest\_raw)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_data_ingestion:ingest_raw.threaded}}\pysiglinewithargsret{\sphinxcode{ingest\_raw.}\sphinxbfcode{threaded}}{\emph{fn}}{}
\end{fulllineitems}



\chapter{Neo4j: API to generate cypher queries}
\label{\detokenize{neo4j_query_generation:neo4j-api-to-generate-cypher-queries}}\label{\detokenize{neo4j_query_generation::doc}}
Here we explain the API to generate cypher queries for Neo4j.


\section{Template of a general query}
\label{\detokenize{neo4j_query_generation:template-of-a-general-query}}

\subsection{Basic Abstraction}
\label{\detokenize{neo4j_query_generation:basic-abstraction}}
Any query can be thought of as a 2 step process -
\begin{itemize}
\item {} 
Extract the relevant sub-graph satisfying the query constraints (Eg. Users and their tweets that use a certain hashtag)

\item {} 
Post-processing of this sub-graph to return desired result (Eg. Return “names” of such users, Return “number” of such users)

\end{itemize}

In a generic way, the 1st step can be constructed using AND,OR,NOT of multiple constraints(Though in our code right now, only AND is supported as Neo4j doesn’t support OR and NOT directly).We now specify how each such constraint can be built.

We look at the network in an abstract in two dimensions.
\begin{itemize}
\item {} 
There are “Entities” (users and tweets) which have “Attributes” (like user has screen\_name,follower\_count etc. and tweet has hashtag,mentions etc.).

\item {} 
The entities have “Relations” between them which have the only attribute as time/time-interval (Eg. Follows “relation” between 2 user “entities” has a time-interval associated).

\end{itemize}

So each constraint can be specified by specifying a pattern consisting of
\begin{itemize}
\item {} 
Two Entities and their Attributes

\item {} 
Relation between the entities and its Attribute (which is the time constraint of this relation)

\end{itemize}

To make things clear we provide an example here.
Suppose our query is - Find users who follow a user with id=1 and have also tweeted with a hashtag “h” between time t1 and t2.
We first break this into AND of two constraints:
\begin{itemize}
\item {} 
User follows a user with id=1

\item {} 
User has tweeted with a hashtag “h” between time t1 and t2.

\end{itemize}

We now specify the 1st constraint using our entity-attribute abstraction.
\begin{itemize}
\item {} 
Source entity - User, Attributes - None

\item {} 
Destination entity - User, Attributes - id=1

\item {} 
Relationship - Follows, Attributes - None

\end{itemize}

We now specify the 2nd constraint using our entity-attribute abstraction.
\begin{itemize}
\item {} 
Source entity - User, Attributes - None

\item {} 
Destination entity - Tweet, Attributes - hashtag:”h”

\item {} 
Relationship - Follows, Attributes - b/w t1,t2

\end{itemize}


\subsection{Naming entities}
\label{\detokenize{neo4j_query_generation:naming-entities}}
The missing thing in this abstraction is that we need to be able to distinguish that the Users in the two constraints above refer to different users. To do so, we “name” each entity (like a variable). So we have:
\begin{itemize}
\item {} \begin{description}
\item[{Constraint 1:}] \leavevmode\begin{itemize}
\item {} 
Source entity - u1:User, Attributes - None

\item {} 
Destination entity - u2:User, Attributes - id=1

\item {} 
Relationship - Follows, Attributes - None

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{Constraint 2:}] \leavevmode\begin{itemize}
\item {} 
Source entity - u1:User, Attributes - None

\item {} 
Destination entity - u3:Tweet, Attributes - hashtag:”h”

\item {} 
Relationship - Follows, Attributes - b/w t1,t2

\end{itemize}

\end{description}

\end{itemize}


\subsection{Variable attributes}
\label{\detokenize{neo4j_query_generation:variable-attributes}}
In the example above, we considered a fixed hashtag “h”. But the user can provide a variable attribute to an entity by giving it a name enclosed in curly braces \sphinxcode{\{\}}. In such a case, the attribute is treated as a variable to the query and the name inside the braces is treated as the name of the said variable. For example, had we input the hashtag as \sphinxcode{\{hash1\}}, it means that our query has a variable named “hash1”.


\subsection{Returns}
\label{\detokenize{neo4j_query_generation:returns}}
The user can write a comma separated list of entities/attributes to return. An entity can be specified directly by its name. Eg. \sphinxcode{my\_tweet\_entity}. An entity’s attribute can be specified as \sphinxcode{\textless{}entitiy\_name\textgreater{}.\textless{}attribute\textgreater{}}. Eg. \sphinxcode{u.screen\_name}.

The user can also group on certain entity/attribute by using an aggregate function on some other entity/attribute. Eg. \sphinxcode{return u.id, count(distinct h)} will group on u.id and return count of distinct h among all records returned for each u.id.


\section{Creating a custom query through dashboard API : Behind the scenes}
\label{\detokenize{neo4j_query_generation:creating-a-custom-query-through-dashboard-api-behind-the-scenes}}
A user can follow the general template of a query as provided above to build a query.
when a user provides the inputs to specify the query, the following steps are executed on the server:
\begin{itemize}
\item {} 
Cleanup and processing of the inputs provided by the user.

\item {} 
The variables(User/Tweet), their attributes and the relations are stored in a database. These stored objects can be later used by the user.

\end{itemize}

Finally, to create the query the user need to specify the query name and the return variables. The query specified by the user in terms of constraints is converted into a Cypher neo4j graph mining query:
\begin{itemize}
\item {} 
All variable attributes are unwinded. This connects to the fact that we are expecting inputs as a list of native objects, hence the unwind for all inputs to the query. This ensures a Cartesian cross in the query.

\item {} 
The time indexed part of the query is generated through the time indexing structure with frames and events.

\item {} 
The network part is generated through the user network and tweet network based on the relationships.

\item {} 
The return variables specified by the user are just concatenated with a return statement in the cypher.

\end{itemize}

It is to be noted here, that we don’t do any kind of checking if the constraints specified by the user to build the query are valid. Checking this in a generic manner without executing the query is difficult. So, this is delegated to the neo4j query engine itself and the user will get empty result in case the constrains are invalid.


\section{Code Documentation for Neo4j query generation}
\label{\detokenize{neo4j_query_generation:code-documentation-for-neo4j-query-generation}}
Here we provide a documentation of the code.
\phantomsection\label{\detokenize{neo4j_query_generation:module-generate_queries}}\index{generate\_queries (module)}
Module to generate cypher code for inputs taken from user though dashboard API.

The {\hyperref[\detokenize{neo4j_query_generation:module-generate_queries}]{\sphinxcrossref{\sphinxcode{generate\_queries}}}} module contains the classes:
\begin{itemize}
\item {} 
{\hyperref[\detokenize{neo4j_query_generation:generate_queries.CreateQuery}]{\sphinxcrossref{\sphinxcode{generate\_queries.CreateQuery}}}}

\end{itemize}

One can use the {\hyperref[\detokenize{neo4j_query_generation:generate_queries.CreateQuery.create_query}]{\sphinxcrossref{\sphinxcode{generate\_queries.CreateQuery.create\_query()}}}} to build a cypher query.

Example illustrating how to create a query which gives the userids and their tweet counts who have used a certian hashtag.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{actors}\PYG{o}{=}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{u}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{USER}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{t}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{TWEET}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{t1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{TWEET}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{attributes}\PYG{o}{=}\PYG{p}{[}\PYG{p}{[}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hashtag}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}hash\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{p}{]}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{relations}\PYG{o}{=}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{u}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{TWEETED}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{t}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{u}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{TWEETED}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{t1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{cq} \PYG{o}{=} \PYG{n}{CreateQuery}\PYG{p}{(}\PYG{p}{)}\PYG{p}{;}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{return\PYGZus{}values}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{u.id,count(t1)}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{ret\PYGZus{}dict} \PYG{o}{=} \PYG{n}{cq}\PYG{o}{.}\PYG{n}{create\PYGZus{}query}\PYG{p}{(}\PYG{n}{actors}\PYG{p}{,}\PYG{n}{attributes}\PYG{p}{,}\PYG{n}{relations}\PYG{p}{,}\PYG{n}{return\PYGZus{}values}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{pprint}\PYG{p}{(}\PYG{n}{ret\PYGZus{}dict}\PYG{p}{,}\PYG{n}{width}\PYG{o}{=}\PYG{l+m+mi}{150}\PYG{p}{)}
\end{sphinxVerbatim}

Example of a query which uses time indexing in a relationship:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{actors} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{USER}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{u1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{USER}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]} \PYG{o}{+} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TWEET}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TWEET}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{attributes} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{12}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{id}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{24}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}\PYG{p}{]}\PYG{o}{+}\PYG{p}{[}\PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{hashtag}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{BLUERISING}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{retweet\PYGZus{}of}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{has\PYGZus{}mention}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{u1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{relations} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{FOLLOWS}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{u1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{TWEETED}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{24}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{48}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}
\index{CreateQuery (class in generate\_queries)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_query_generation:generate_queries.CreateQuery}}\pysigline{\sphinxbfcode{class }\sphinxcode{generate\_queries.}\sphinxbfcode{CreateQuery}}
Bases: \sphinxcode{object}

Class containing functions to generate query.
\index{conditional\_create() (generate\_queries.CreateQuery method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_query_generation:generate_queries.CreateQuery.conditional_create}}\pysiglinewithargsret{\sphinxbfcode{conditional\_create}}{\emph{entity}}{}
Condionally provide the attributes of the node if not already created, else directly use the name of the variable create earlier.
If already create, pass empty list of properties.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{entity} \textendash{} the entity which to check and create

\item[{Returns}] \leavevmode
the code for the node as neo4j node enclosed in ()

\end{description}\end{quote}

\end{fulllineitems}

\index{create\_query() (generate\_queries.CreateQuery method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_query_generation:generate_queries.CreateQuery.create_query}}\pysiglinewithargsret{\sphinxbfcode{create\_query}}{\emph{actors}, \emph{attributes}, \emph{relations}, \emph{return\_values}}{}
Takes a list of attributes and relationships between them and return a cypher code as string.
For the format of the lists see the examples.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{actors} \textendash{} the variable names, types of the attributes

\item {} 
\sphinxstyleliteralstrong{attributes} \textendash{} the properties of the actors

\item {} 
\sphinxstyleliteralstrong{relations} \textendash{} the relations between the entities along with time index for the relations

\item {} 
\sphinxstyleliteralstrong{return\_values} \textendash{} a direct string containing the return directive.

\end{itemize}

\item[{Returns}] \leavevmode
index of the bond in the molecule

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
Here we are expecting that if user has not specified the times on the dashboard, then we pass epmty string. If you
store some other default in dashboard database then change this accordingly.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
The return \_values is directly used as a string in the cypher query, so the user can use AS and other similar cypher directives while specifying the query.
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Todo:}
Support to compose queries using OR. For example, currently compostion of relationships or attribute properties like all tweets(t) which are retweets of t1 or quoted t2, is not supported. Use cypher union for this.
\end{sphinxadmonition}

\end{fulllineitems}

\index{generate\_node() (generate\_queries.CreateQuery method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{neo4j_query_generation:generate_queries.CreateQuery.generate_node}}\pysiglinewithargsret{\sphinxbfcode{generate\_node}}{\emph{var}, \emph{type}, \emph{props}}{}
Helper function for {\hyperref[\detokenize{neo4j_query_generation:generate_queries.CreateQuery.conditional_create}]{\sphinxcrossref{\sphinxcode{generate\_queries.CreateQuery.conditional\_create()}}}}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{var} \textendash{} the variable name of the entity

\item {} 
\sphinxstyleliteralstrong{type} \textendash{} the type of the entity. Observe we pass type as :USER and NOT as USER

\item {} 
\sphinxstyleliteralstrong{props} \textendash{} the properties of the entity.

\end{itemize}

\item[{Returns}] \leavevmode
the code for the node as neo4j node enclosed in ()

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\chapter{Generating queries in mongoDB}
\label{\detokenize{mongoDB_query_generation:generating-queries-in-mongodb}}\label{\detokenize{mongoDB_query_generation::doc}}
As mentioned in mongoDB ingestion section, in mongoDB we store only the data without any structural information which can be extracted quickly from incoming tweets without much processing. Further, we store in mongoDB to ensure that some very common queries can be answered quickly.

This leads to these important properties of the mongoDB part of datastore:
\begin{itemize}
\item {} 
Only very specific queries can be answered using only the mongoDB. The specific queries further depend on the data which is being stored, which is further decided by which queries are seen frequently and need to be sped up. Given that currently we have the hashtag, url and user mention collection in mongoDB with the entity name, timestamp, the sentiment associated with the tweet in which the mentioned entity occurred; we can answer only specific queries like the most popular hashtag(and its sentiment) occurring in an interval(which can be the entire time as well).

\item {} 
This further means that the mongoDB schema and datastore can easily be modified and extended. For example, if I decide to store the named entities in the tweets as well, all we need is to make a new collection.

\end{itemize}

Contrast this with neo4j where the schema is more or less for fixed for all practical purposes and the user can’t easily change it.

Given the above two properties, it makes little sense to develop a complete generic API to input queries from the user as it would certainly be an overkill. So currently we provide APIs only to take only specific queries from the user. The queries which can currently be answered using mongoDB are follows(the things mentioned inside \textless{}\textgreater{} are the inputs to the query):
\begin{itemize}
\item {} 
Give the \textless{}number\textgreater{} most popular hashtags in total

\item {} 
Give the \textless{}number\textgreater{} most popular hashtags in the time interval \textless{}Begin Time\textgreater{} and \textless{}End Time\textgreater{}

\item {} 
Give the timetamps at which \textless{}hashtag\textgreater{} is used between \textless{}Begin Time\textgreater{} and \textless{}End Time\textgreater{}

\item {} 
Give the timetamps, associated positive and negative sentiment of a \textless{}hashtag\textgreater{} between \textless{}Begin Time\textgreater{} and \textless{}End Time\textgreater{}

\end{itemize}

Similarly, queries analogous to the above can also be answered for urls and user mentions.


\section{Generic API for mongoDB : Idea}
\label{\detokenize{mongoDB_query_generation:generic-api-for-mongodb-idea}}
For sake of completeness we also provide the way to generate a generic API to get mongoDB queries. For example take at this code to answer the query to get the most popular hashtags in an interval:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pipeline} \PYG{o}{=} \PYG{p}{[}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}match}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{timestamp}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}gte}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{n}{t1}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}lte}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{n}{t2}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}group}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZus{}id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}hashtag}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{count}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}sum}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{1}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}sort}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{count}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdl{}limit}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{n}{limit}\PYG{p}{\PYGZcb{}}\PYG{p}{]}
\PYG{n}{l} \PYG{o}{=}  \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{db}\PYG{o}{.}\PYG{n}{ht\PYGZus{}collection}\PYG{o}{.}\PYG{n}{aggregate}\PYG{p}{(}\PYG{n}{pipeline}\PYG{p}{)}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{result}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{k}{return} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{hashtag}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{p}{[}\PYG{n}{x}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZus{}id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{l}\PYG{p}{]}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{count}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{p}{[}\PYG{n}{x}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{count}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{l}\PYG{p}{]}\PYG{p}{\PYGZcb{}}
\end{sphinxVerbatim}

As can be seen the query answering has three parts :
\begin{itemize}
\item {} 
The aggregation pipeline: This is the part with needs to be input from the user. As we have limited number of constructs that, can be used in aggregation, Taking those as inputs along with their parameters is not that difficult. Some of the construct, though can also be filled in automatically.

\item {} 
Aggregating the collection based on the pipeline: This has fixed code and can be generated easily.

\item {} 
Unzipping the result based on the output variables name input by the user: Again, fixed code and thus easily generated.

\end{itemize}

Along with the reasons mentioned above regarding the non requirement of generic monogDB query creator, another reason is that to generate the queries, would invariably require generation of python code, something like the above snippet and then modifying a file with a new function to connect to the database and execute the python code. This would further require modifying the source code in the dashboard website and the DAG execution python functions as well to register the new function, opening several fronts from which bugs can creep in.


\subsection{MongoDB query execution code documentation}
\label{\detokenize{mongoDB_query_generation:mongodb-query-execution-code-documentation}}
Here we provide a documentation of the code used for this functionality.
\phantomsection\label{\detokenize{mongoDB_query_generation:module-execute_queries}}\index{execute\_queries (module)}
Module to execute mongoDB queries. The idea is to keep the  mongo interface minimal and easily extensible, and thus
only pre specified queries can be answered through mongoDB, rather than generic ones.

The {\hyperref[\detokenize{mongoDB_query_generation:module-execute_queries}]{\sphinxcrossref{\sphinxcode{execute\_queries}}}} module contains the classes:
\begin{itemize}
\item {} 
{\hyperref[\detokenize{mongoDB_query_generation:execute_queries.MongoQuery}]{\sphinxcrossref{\sphinxcode{execute\_queries.MongoQuery}}}}

\end{itemize}

One can use the different function in the class to execute different queries

Example illustrating how to answer different queries.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{q} \PYG{o}{=} \PYG{n}{MongoQuery}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{q}\PYG{o}{.}\PYG{n}{mp\PYGZus{}ht\PYGZus{}in\PYGZus{}total}\PYG{p}{(}\PYG{n}{limit}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} get 10 most popular hashtags}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{q}\PYG{o}{.}\PYG{n}{mp\PYGZus{}um\PYGZus{}in\PYGZus{}total}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} get 10 most popular users}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{q}\PYG{o}{.}\PYG{n}{mp\PYGZus{}ht\PYGZus{}in\PYGZus{}interval}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{1500486521}\PYG{p}{,}\PYG{l+m+mi}{1501496521}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} get 10 most popular hashtags in interval}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{q}\PYG{o}{.}\PYG{n}{ht\PYGZus{}in\PYGZus{}interval}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{baystars}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+m+mi}{1500486521}\PYG{p}{,}\PYG{l+m+mi}{1501496521}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} get the timestamps at which baystars is used in interval}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{q}\PYG{o}{.}\PYG{n}{ht\PYGZus{}with\PYGZus{}sentiment}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{baystars}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+m+mi}{1500486521}\PYG{p}{,}\PYG{l+m+mi}{1501496521}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} get the timestamps and sentiment at which baystars is used in interval}
\end{sphinxVerbatim}
\index{MongoQuery (class in execute\_queries)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_query_generation:execute_queries.MongoQuery}}\pysigline{\sphinxbfcode{class }\sphinxcode{execute\_queries.}\sphinxbfcode{MongoQuery}}
Bases: \sphinxcode{object}

Class to answer mongoDB queries. Make connection to the database and keep on answering queris untill the
object is deleted
\index{clear\_db() (execute\_queries.MongoQuery method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_query_generation:execute_queries.MongoQuery.clear_db}}\pysiglinewithargsret{\sphinxbfcode{clear\_db}}{}{}
Delete all the collections

\end{fulllineitems}

\index{ht\_in\_interval() (execute\_queries.MongoQuery method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_query_generation:execute_queries.MongoQuery.ht_in_interval}}\pysiglinewithargsret{\sphinxbfcode{ht\_in\_interval}}{\emph{hashtag}, \emph{begin}, \emph{end}}{}
Give the timetamps at which \textless{}hashtag\textgreater{} is used between \textless{}begin\textgreater{} and \textless{}end\textgreater{}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{hashtag} \textendash{} hashtag for the query

\item {} 
\sphinxstyleliteralstrong{begin} \textendash{} the begining unix time timestamp of the interval

\item {} 
\sphinxstyleliteralstrong{end} \textendash{} the ending unix time timestamp of the interval

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{ht\_with\_sentiment() (execute\_queries.MongoQuery method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_query_generation:execute_queries.MongoQuery.ht_with_sentiment}}\pysiglinewithargsret{\sphinxbfcode{ht\_with\_sentiment}}{\emph{hashtag}, \emph{begin}, \emph{end}}{}
Give the timetamps at which \textless{}hashtag\textgreater{} is used and and sentiment of tweet in which \textless{}hashtag\textgreater{} occured between \textless{}begin\textgreater{} and \textless{}end\textgreater{}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{hashtag} \textendash{} hashtag for the query

\item {} 
\sphinxstyleliteralstrong{begin} \textendash{} the begining unix time timestamp of the interval

\item {} 
\sphinxstyleliteralstrong{end} \textendash{} the ending unix time timestamp of the interval

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{mp\_ht\_in\_interval() (execute\_queries.MongoQuery method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_query_generation:execute_queries.MongoQuery.mp_ht_in_interval}}\pysiglinewithargsret{\sphinxbfcode{mp\_ht\_in\_interval}}{\emph{limit}, \emph{begin}, \emph{end}}{}
Function to give the most popular hashtags in the time interval \textless{}begin\textgreater{} and \textless{}end\textgreater{}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{limit} \textendash{} number of records to return

\item {} 
\sphinxstyleliteralstrong{begin} \textendash{} the begining unix time timestamp of the interval

\item {} 
\sphinxstyleliteralstrong{end} \textendash{} the ending unix time timestamp of the interval

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{mp\_ht\_in\_total() (execute\_queries.MongoQuery method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_query_generation:execute_queries.MongoQuery.mp_ht_in_total}}\pysiglinewithargsret{\sphinxbfcode{mp\_ht\_in\_total}}{\emph{limit}}{}
Give \textless{}limit\textgreater{} most popular hashtags in total
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{limit} \textendash{} number of records to return

\end{description}\end{quote}

\end{fulllineitems}

\index{mp\_um\_in\_total() (execute\_queries.MongoQuery method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{mongoDB_query_generation:execute_queries.MongoQuery.mp_um_in_total}}\pysiglinewithargsret{\sphinxbfcode{mp\_um\_in\_total}}{\emph{limit}}{}
Give \textless{}limit\textgreater{} most popular users(in iterms of mentions) in total
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{limit} \textendash{} number of records to return

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\chapter{About Post processing functions}
\label{\detokenize{postprocessing:about-post-processing-functions}}\label{\detokenize{postprocessing::doc}}

\section{Need of post processing function}
\label{\detokenize{postprocessing:need-of-post-processing-function}}
Some processing can be done in a cypher query in case of neo4j, and further in case of mongoDB, there is functionality to write custom functions to be included in the aggregation pipeline. But we provide the user the ability to create post processing function. The major reasons behind this are:
\begin{itemize}
\item {} 
It may be easy to do some projection on data output by a query post the execution, rather than coding it in the cypher in case of neo4j, or the aggregation pipeline in case of mongoDB.

\item {} 
On similar lines as above, the user may need to aggregate multiple outputs from different queries in a post processing function in a custom manner not supported by the query mechanism of the databases.

\end{itemize}


\section{Format of post processing functions}
\label{\detokenize{postprocessing:format-of-post-processing-functions}}
We treat the post processing functions as queries. The idea behind treating post processing functions as a query is to provide simplistic abstraction while creating a DAG. Thus while creating a DAG, a user has to just compose queries which can either be query to any of the two databases or a post processing function as well. Thus, given the DAG abstraction, the user can feed the output of the query(ies) into a post processing node.

Further to support this abstraction, we require the post processing function to accept a dictionary of lists of native python objects(named “inputs”) and return a dictionary(named “ret”) in same format. The function should further be named as “func”. This requires that the user specifies the input and output variable names while creating the post processing function. This will be explained in detail in the DAG section.

Another way(instead of asking the user to explicitly provide the input and output variable names) in which post processing function could have been created is to just take as input the code of the function, parse it to get the number of inputs and their names. This is relatively easy. But, the issue is to get the output variables. This is a difficult problem and exactly this is used to generate automatic documentation of python code. But has been observed, even it misses the names of return variables. Its easy in case named variables are returned but the issue is when expressions are returned(for example the code contains \sphinxcode{return l{[}:10{]}}, its not clear what should be the name of the return variable). Thus, out adopted method of dealing with dictionaries with named variables provides a clean abstraction over the the alternative.

Here is an example of a post processing function to output the union of lists input to it:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{func}\PYG{p}{(}\PYG{n+nb}{input}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Function to take union of two lists.}
\PYG{l+s+sd}{    :param: input \PYGZhy{} a dictionary with the attribute names as keys and their values as dict\PYGZhy{}values.}
\PYG{l+s+sd}{    :return: a dictionary with output variables as keys.}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{l1} \PYG{o}{=} \PYG{n+nb}{input}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{list1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
    \PYG{n}{l2} \PYG{o}{=} \PYG{n+nb}{input}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{list2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{l2}\PYG{p}{:}
        \PYG{k}{if} \PYG{n}{x} \PYG{o+ow}{not} \PYG{o+ow}{in} \PYG{n}{l1}\PYG{p}{:}
            \PYG{n}{l1}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}

    \PYG{n}{ret} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
    \PYG{n}{ret}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{l\PYGZus{}out}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{l1}
    \PYG{k}{return} \PYG{n}{ret}
\end{sphinxVerbatim}

Post processing functions are also used to display custom metric. To view a custom metric, the user is required to specify a post processing function which accepts as inputs the outputs of any of the queries in the DAG and outputs a x and y coordinates to be used for plotting.

Here is another example of a post processing function to create a custom metric to plot the users with their number of tweets:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{func}\PYG{p}{(}\PYG{n}{inputs}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{inputs} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{inputs}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{userid}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{inputs}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{count}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{inputs}\PYG{o}{.}\PYG{n}{sort}\PYG{p}{(}\PYG{n}{key}\PYG{o}{=}\PYG{k}{lambda} \PYG{n}{item}\PYG{p}{:}\PYG{n}{item}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{reverse}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{)}
    \PYG{n}{x\PYGZus{}vals} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{y\PYGZus{}vals} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x\PYGZus{}vals}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n+nb}{str}\PYG{p}{(}\PYG{n}{inputs}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{y\PYGZus{}vals}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{inputs}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}

    \PYG{n}{ret} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
    \PYG{n}{ret}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{x\PYGZus{}vals}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{x\PYGZus{}vals}
    \PYG{n}{ret}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{y\PYGZus{}vals}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{y\PYGZus{}vals}
    \PYG{k}{return} \PYG{n}{ret}
\end{sphinxVerbatim}


\section{Executing post processing function}
\label{\detokenize{postprocessing:executing-post-processing-function}}
To execute the post processing function, we just provide include the inputs in the context being passed to the function. The execution requires these three steps:
\begin{itemize}
\item {} 
Compilation : Any code errors are output to the user at this point.

\item {} 
Passing the inputs to the function and executing its code.

\item {} 
Obtaining the outputs : The output ret dictionary is pushed on the context by the function.

\end{itemize}

This can be seen in this code snippet used to execute the post processing functions:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{context} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{inputs}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{n}{copy}\PYG{o}{.}\PYG{n}{deepcopy}\PYG{p}{(}\PYG{n}{inputs}\PYG{p}{)}\PYG{p}{\PYGZcb{}}
\PYG{k}{try}\PYG{p}{:}
    \PYG{n+nb}{compile}\PYG{p}{(}\PYG{n}{function\PYGZus{}code}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{exec}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{k}{exec}\PYG{p}{(}\PYG{n}{functiona\PYGZus{}code} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{+} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ret = func(inputs)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{context}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{out} \PYG{o+ow}{in} \PYG{n}{outputs}\PYG{p}{:}
        \PYG{n}{ret}\PYG{p}{[}\PYG{n}{out}\PYG{p}{]} \PYG{o}{=} \PYG{n}{context}\PYG{p}{[}\PYG{n}{out}\PYG{p}{]}
\PYG{k}{except} \PYG{n+ne}{Exception} \PYG{k}{as} \PYG{n}{e}\PYG{p}{:}
    \PYG{k}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Exception while executing Post proc function: }\PYG{l+s+si}{\PYGZpc{}s}\PYG{l+s+s2}{, }\PYG{l+s+si}{\PYGZpc{}s}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{\PYGZpc{}}\PYG{p}{(}\PYG{n+nb}{type}\PYG{p}{(}\PYG{n}{e}\PYG{p}{)}\PYG{p}{,}\PYG{n}{e}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}


\chapter{Composing multiple queries : DAG}
\label{\detokenize{dag:composing-multiple-queries-dag}}\label{\detokenize{dag::doc}}

\section{Basic terminology}
\label{\detokenize{dag:basic-terminology}}
When we say \sphinxstylestrong{Query}, it means an one of the following three things:
\begin{itemize}
\item {} 
MongoDB query : A query not capable of giving any network information

\item {} 
Neo4j query : A network based and/or time indexed query on the twitter network

\item {} 
Post processing function : A python function which takes outputs of query(ies) as inputs and transforms them to give the output

\end{itemize}

\sphinxstylestrong{DAG} stands for directed acyclic graph. Thus it a directed graph with no cycles. The idea behind a DAG is to compose multiple queries to build a complex queries. A DAG has nodes and has directed connections connections between the nodes. Each node as a query associated with it.


\section{Idea behind a DAG}
\label{\detokenize{dag:idea-behind-a-dag}}
As mentioned above, our main idea is to provide the user an easy abstraction to build complex queries. But apart from this there are several functions that the abstraction of a DAG seems to serve, which we list below:
\begin{itemize}
\item {} 
Provide an abstraction to build complex queries from simple queries.

\item {} 
A particular database may be suited to answer particular type of queries. In fact this is the main reason behind storing data in mongoDB to answer commonly encountered queries. We expect the user to have a basic understanding of the database schemas and thus be able to have an idea of efficiency of the two databases in answering specific queries. Having such knowledge, the user can compose queries from different databases in sake of efficiency.

\item {} 
It may be easy to do some projection of data output by a query post the execution, rather than coding it in the cypher in case of neo4j, or the aggregation pipeline in case of mongoDB. Thus, given the DAG abstraction, the user can feed the output of the query into a postprocessing node.

\item {} 
On similar lines as above, the user may need to aggregate multiple outputs from different queries in a postprocessing function in a custom manner not supported by the query mechanism of the databases.

\item {} 
Breaking a big query into smaller ones may be beneficial from the end user point of view because by doing so we can show the incremental results of the smaller parts(as they are executed) to the user instead of waiting for the entire big query to execute.

\end{itemize}

In this abstraction, a single query can also be treated as a DAG, one having a single node and no connections.

We store the queries that the user creates through the dashboard. The user can then specify the structure of the DAG network by uploading a file in which he specifies how outputs and inputs of queries are connected. We provide the details in the next section.


\section{Building a DAG from queries}
\label{\detokenize{dag:building-a-dag-from-queries}}
A DAG is composition of queries in which we need to specify how the outputs of queries upstream feed into the inputs of the downstream ones.

We explain how to build the queries with the help on an example. Let us build a DAG to get the most active users. Refer to this image(the green queries represent mongoDB queries and blue ones represent neo4j queries):

\noindent\sphinxincludegraphics{{example_query}.jpg}

First we need to build the three queries separately, let us say we have the built queries as:
\begin{itemize}
\item {} \begin{description}
\item[{mongoDB query(most\_popular\_hashtags\_20 - Node 1) - 20 most popular hashtags in total}] \leavevmode\begin{itemize}
\item {} 
INPUTS  : limit(number of records to return)

\item {} 
OUTPUTS : hashtags(list of popular hashtags, arranged by count in decreasing order), counts(list of their corresponding counts)

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{mongoDB query(most\_popular\_mentions\_20 - Node 2) - 20 most popular users(in terms of number of mentions) in total}] \leavevmode\begin{itemize}
\item {} 
INPUTS  : limit(number of records to return)

\item {} 
OUTPUTS : user\_metions(list of popular users, arranged by count in decreasing order), counts(list of their corresponding counts)

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{neo4j query(active\_users - Node 3)   - userIds and their tweet counts who have used one of the popular hashtags atleast once and have tweeted with one of the popular user mentions atleast once}] \leavevmode\begin{itemize}
\item {} 
INPUTS  : hash\_in(list of 20 most popular hashtags), users\_in(list of 20 most popular users)

\item {} 
OUTPUTS : userIds(list of required users), tweet\_counts(total number of their tweets)

\end{itemize}

\end{description}

\end{itemize}

This query is demonstrated by the block diagram below also:

\noindent\sphinxincludegraphics{{example_query_detailed}.jpg}

As mentioned in neo4j query generation section, we expect all the inputs to the neo4j query to be  list of native objects. We put a similar constraint on the inputs to post processing function. Keeping this in mind, to ensure consistency and a seamless flow of information, the outputs of each query(mongoDB, neo4j or postprocessing function) is expected to be a list. Thus each node in the DAG accepts a dictionary as input in which the values are lists and similarly returns a dictionary with list values. The keys in both dictionary is the name of the inputs/outputs, as specified in the query generation.

The only place where the list input breaks is in case of mongoDB query as they require some basic inputs which can directly be provided as native objects(for example the limit input to the above two mongoDB queries).

Further we need to specify which outputs of the queries are to be returned.

The example input file to create the above DAG looks something like this:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mi}{3}
\PYG{n}{n1} \PYG{n}{most\PYGZus{}popular\PYGZus{}hashtags\PYGZus{}20}
\PYG{n}{n2} \PYG{n}{most\PYGZus{}popular\PYGZus{}mentions\PYGZus{}20}
\PYG{n}{n3} \PYG{n}{active\PYGZus{}users}
\PYG{n}{INPUTS}\PYG{p}{:}
\PYG{n}{CONNECTIONS}\PYG{p}{:}
\PYG{n}{n1}\PYG{o}{.}\PYG{n}{hashtag} \PYG{n}{n3}\PYG{o}{.}\PYG{n}{hashtag}
\PYG{n}{n2}\PYG{o}{.}\PYG{n}{userId} \PYG{n}{n3}\PYG{o}{.}\PYG{n}{um\PYGZus{}id}
\PYG{n}{RETURNS}\PYG{p}{:}
\PYG{n}{n3}\PYG{o}{.}\PYG{n}{userId}
\PYG{n}{n3}\PYG{o}{.}\PYG{n}{count}
\end{sphinxVerbatim}

Observe the structure of the file.
\begin{itemize}
\item {} 
Line 1 contains the number of nodes in the DAG.

\item {} 
The following number of nodes lines contain the name of the nodes and  each node corresponds to which query.

\item {} 
Then a line contains the keyword “INPUTS:”. The inputs to the queries in the DAG are specified here. For example, had the \sphinxcode{n1.hashtag} variable been taken as an input rather than feeding from the output of an upstream query, it would have been specified as \sphinxcode{n1.hashtag {[}"hash1","hash2"{]}}.

\item {} 
Then a line containing the keyword “CONNECTION:”. Below the connections in the DAG are specified.

\item {} 
And finally a line contains the keyword “RETURNS:”. Below, we specify the outputs of the queries which are to be returned.

\end{itemize}

Please note that, all the outputs of all the queries can be seen in XComs in airflow and also in the logs of the DAG run. But we provide the user to specify the things of interest to the user, through the RETURN variables. This will be useful in case we provide a functionality to observe the variation of a quantity over periodic DAG runs in future. Presently we don’t have such a functionality in our Dashboard, though providing such a functionality shouldn’t be difficult.


\section{DAG in airflow}
\label{\detokenize{dag:dag-in-airflow}}
To create a DAG in airflow, we need to create a file in the dags folder in the AIRFLOW\_HOME directory. So, when the user specifies the DAG through out dashboard, we generate a python file in the mentioned folder. The newly created DAG is registered with airflow after sometime(airflow has a heartbeat thread running, which looks for new DAGs in the folder periodically)
We generate the code to specify the dag in airflow something like this.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{task\PYGZus{}0} \PYG{o}{=} \PYG{n}{PythonOperator}\PYG{p}{(}
    \PYG{n}{task\PYGZus{}id}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{node\PYGZus{}\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{python\PYGZus{}callable}\PYG{o}{=}\PYG{n}{execute\PYGZus{}query}\PYG{p}{,}
    \PYG{n}{op\PYGZus{}kwargs}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{node\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
    \PYG{n}{provide\PYGZus{}context} \PYG{o}{=} \PYG{n+nb+bp}{True}\PYG{p}{,}
    \PYG{n}{dag}\PYG{o}{=}\PYG{n}{dag}\PYG{p}{)}

\PYG{n}{task\PYGZus{}1} \PYG{o}{=} \PYG{n}{PythonOperator}\PYG{p}{(}
        \PYG{n}{task\PYGZus{}id}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{node\PYGZus{}\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
        \PYG{n}{python\PYGZus{}callable}\PYG{o}{=}\PYG{n}{execute\PYGZus{}query}\PYG{p}{,}
        \PYG{n}{op\PYGZus{}kwargs}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{node\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
        \PYG{n}{provide\PYGZus{}context} \PYG{o}{=} \PYG{n+nb+bp}{True}\PYG{p}{,}
        \PYG{n}{dag}\PYG{o}{=}\PYG{n}{dag}\PYG{p}{)}

\PYG{n}{task\PYGZus{}2} \PYG{o}{=} \PYG{n}{PythonOperator}\PYG{p}{(}
        \PYG{n}{task\PYGZus{}id}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{node\PYGZus{}\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n3}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{,}
        \PYG{n}{python\PYGZus{}callable}\PYG{o}{=}\PYG{n}{execute\PYGZus{}query}\PYG{p}{,}
        \PYG{n}{op\PYGZus{}kwargs}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{node\PYGZus{}name}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{n3}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{,}
        \PYG{n}{provide\PYGZus{}context} \PYG{o}{=} \PYG{n+nb+bp}{True}\PYG{p}{,}
        \PYG{n}{dag}\PYG{o}{=}\PYG{n}{dag}\PYG{p}{)}
\PYG{n}{task\PYGZus{}0} \PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{task\PYGZus{}2}
\PYG{n}{task\PYGZus{}1} \PYG{o}{\PYGZgt{}\PYGZgt{}} \PYG{n}{task\PYGZus{}2}
\end{sphinxVerbatim}

In the above code, the execute query is the function in which we execute queries and pass on their outputs to XComs to be used by the downstream nodes.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Pushing onto XComs}
\PYG{n}{context}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{task\PYGZus{}instance}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{xcom\PYGZus{}push}\PYG{p}{(}\PYG{n}{k}\PYG{p}{,}\PYG{n}{v}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} Pulling from XComs}
\PYG{n}{context}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{task\PYGZus{}instance}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{xcom\PYGZus{}pull}\PYG{p}{(}\PYG{n}{task\PYGZus{}ids}\PYG{o}{=}\PYG{n}{get\PYGZus{}task\PYGZus{}from\PYGZus{}node}\PYG{p}{(}\PYG{n}{mapp}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}\PYG{n}{dag\PYGZus{}id} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{active\PYGZus{}users\PYGZus{}dag}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{n}{key}\PYG{o}{=}\PYG{n}{k}\PYG{p}{)}
\end{sphinxVerbatim}

Apart from this, some of the DAG properties which needs to be specified in airflow are generated as the default ones. For example the \sphinxcode{start\_date} property is specified as the current time.

Airflow provides certain other useful properties which may be of interest to the user(when the system becomes huge). For example, the user can set an email-address on which a notification will be sent in case of the success and/or failure of tasks. But, taking all these inputs though our dashboard to generate the DAG, is like create a complete front-end wrapper over the airflow system, which is not out aim. If the user does wish to use the more involved airflow properties, he/she can always edit the source of the generated dag file. Also, if the need arises to provide the user such functionality through our dashboard, then modifying the code to generate an additional line in the dag python file is easy.

Further on airflow, different views of the DAG can be observed, some of the views which are of particular interest to us are the following :
\begin{itemize}
\item {} 
Tree view - A view which tells the parallel streams in the DAG. We can specify how many parallel worker threads to have in airflow.

\item {} 
Graph view - A graph view specifying the connections between the nodes of the DAG.

\item {} 
Gant view - activates after the DAG has been executed, tells how much time taken by each query to execute.

\end{itemize}

Also,airflow provides the functionality to schedule the DAG runs periodically and properly stores the logs of each run. This can be leveraged in scenarios in which the user wants to run the same compositional query periodically.


\section{Creating custom metric}
\label{\detokenize{dag:creating-custom-metric}}
Custom metric can be created on top of the DAG. A custom metric is nothing but a graphical view of the data output from the DAG execution.

To view a custom metric, the user is required to specify the following things:
\begin{itemize}
\item {} 
A DAG : The outputs of any queries in the DAG can be used to create the custom metric.

\item {} 
A post processing function : accepts as inputs the outputs of any of the queries in the DAG and outputs a x and y coordinates to be used for plotting.

\item {} 
Either mapping between the inputs of the post processing function and the outputs of the queries in the DAG or fixed native values to the inputs.

\end{itemize}

To display the custom metric, the DAG is executed to feed data into the post processing function. The user can choose to view the metric in either of these formats:
\begin{itemize}
\item {} 
Plot : The x and y coordinates are plotted using plotly through an Ajax call and displayed on the dashboard.

\item {} 
Table : The values are displayed in table format again using an Ajax call.

\end{itemize}

An example of creating a custom metric will be provided in the {\hyperref[\detokenize{dashboard_website:dashboard-website}]{\sphinxcrossref{\DUrole{std,std-ref}{Dashboard Website}}}} section.


\section{Code Documentation for DAG abstraction}
\label{\detokenize{dag:code-documentation-for-dag-abstraction}}
Here we provide a documentation of the code used to generate, execute the DAG.
\phantomsection\label{\detokenize{dag:module-create_dag}}\index{create\_dag (module)}
Module to generate and store the DAG created by the user. Constains functions to generate DAG for the airflow
dashboard also.

The {\hyperref[\detokenize{dag:module-create_dag}]{\sphinxcrossref{\sphinxcode{create\_dag}}}} module contains the classes:
\begin{itemize}
\item {} 
{\hyperref[\detokenize{dag:create_dag.DAG}]{\sphinxcrossref{\sphinxcode{create\_dag.DAG}}}}

\end{itemize}

When we instantiate an object of the class, the network source of the DAG is parsed to get parameters.
One can use the function {\hyperref[\detokenize{dag:create_dag.DAG.feed_forward}]{\sphinxcrossref{\sphinxcode{create\_dag.DAG.feed\_forward()}}}} to execute the networkx DAG and the
function {\hyperref[\detokenize{dag:create_dag.DAG.generate_dag}]{\sphinxcrossref{\sphinxcode{create\_dag.DAG.generate\_dag()}}}} to generate an airflow DAG.

Example illustrating how to create a DAG  in which no input to no query is constant:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{queries} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{q1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{query 1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{inp1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{inp2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{out1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{out2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,}
\PYG{g+go}{                        \PYGZdq{}q2\PYGZdq{}:[\PYGZdq{}query 2\PYGZdq{},[\PYGZdq{}inp1\PYGZdq{}],[\PYGZdq{}out1\PYGZdq{},\PYGZdq{}out2\PYGZdq{}]],}
\PYG{g+go}{                        \PYGZdq{}q3\PYGZdq{}:[\PYGZdq{}query 3\PYGZdq{},[\PYGZdq{}inp1\PYGZdq{},\PYGZdq{}inp2\PYGZdq{},\PYGZdq{}inp3\PYGZdq{}],[\PYGZdq{}out1\PYGZdq{}]]\PYGZcb{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{types} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{q1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{mongoDB}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{q2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{PostProcesing}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{q3}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{neo4j}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{constants} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{c+c1}{\PYGZsh{} input\PYGZus{}graph.txt contains the network of the DAG. For format see the documentation}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{dag} \PYG{o}{=} \PYG{n}{DAG}\PYG{p}{(}\PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{input\PYGZus{}graph.txt}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{r}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{read}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{queries}\PYG{p}{,}\PYG{n}{types}\PYG{p}{,}\PYG{n}{constants}\PYG{p}{)}
\end{sphinxVerbatim}

Example of a query in which we use constants:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{c+c1}{\PYGZsh{} queries and types as before}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{constants} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{q1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{inp1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}\PYG{l+m+mi}{3}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZcb{}}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{c+c1}{\PYGZsh{} input\PYGZus{}graph.txt contains the network of the DAG. For format see the documentation}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{dag} \PYG{o}{=} \PYG{n}{DAG}\PYG{p}{(}\PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{input\PYGZus{}graph.txt}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{r}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{read}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}\PYG{n}{queries}\PYG{p}{,}\PYG{n}{types}\PYG{p}{,}\PYG{n}{constants}\PYG{p}{)}
\end{sphinxVerbatim}

Now to execute the graph provide a function which can execute the queries.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{dag}\PYG{o}{.}\PYG{n}{feed\PYGZus{}forward}\PYG{p}{(}\PYG{o}{\PYGZlt{}}\PYG{n}{execute}\PYG{o}{\PYGZgt{}}\PYG{p}{)}
\end{sphinxVerbatim}

Create a DAG in airflow, get the plotly div for the DAG to be displayed on the dashboard

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{dag}\PYG{o}{.}\PYG{n}{generate\PYGZus{}dag}\PYG{p}{(}\PYG{o}{\PYGZlt{}}\PYG{n}{dag\PYGZus{}name}\PYG{o}{\PYGZgt{}}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{dag}\PYG{o}{.}\PYG{n}{plot\PYGZus{}dag}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}
\index{DAG (class in create\_dag)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{dag:create_dag.DAG}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{create\_dag.}\sphinxbfcode{DAG}}{\emph{network\_file\_source}, \emph{queries}, \emph{types}, \emph{constants}}{}
Bases: \sphinxcode{object}

This class contains the functions to deal with the abstraction of DAG in our system. Some of the functions
to this end, some of the functions are in the views.py file also.

The \_\_init\_\_ is called to initilaize a DAG object. It reads the source of the DAG network file passed to it as a string. It
parses the string and extracts te nodes, connections, inputs and returns from the file.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{network\_file\_source} \textendash{} a string contains the network specificaion of the DAG

\item {} 
\sphinxstyleliteralstrong{queries} \textendash{} a dictionary of queries with keys as the query names/postprocessing function names

\item {} 
\sphinxstyleliteralstrong{types} \textendash{} a dictionary containing the types of different queries

\item {} 
\sphinxstyleliteralstrong{constants} \textendash{} a dictionary of constant inputs to a query

\end{itemize}

\end{description}\end{quote}

For the queries parameter, the expectation depends on the type of query:
\begin{itemize}
\item {} 
For neo4j queries it will be the query code, input, output list

\item {} 
For mongoDB queries it will be the partially formatted query specification, input, output list

\item {} 
For post processing functions it will be the function\_definition, input, output list

\end{itemize}

\begin{sphinxadmonition}{note}{Note:}
The constants dictionary will contain only the mongoDB queries in current format
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
We don’t do very comprehensive checking if the network is valid.
In case it is not a DAG, the user is notified of it. Other than that, there will be some python errors if some other issue is there.
\end{sphinxadmonition}
\index{feed\_forward() (create\_dag.DAG method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{dag:create_dag.DAG.feed_forward}}\pysiglinewithargsret{\sphinxbfcode{feed\_forward}}{\emph{execute}}{}
Do a topological sort and then do a BFS of the DAG to execute all the queries.
Expect that there is a function to evaluate a query given its inputs(as a dictionary) and returns a dictionary of outputs
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{execute} \textendash{} a function to execute the queries

\end{description}\end{quote}

\end{fulllineitems}

\index{generate\_dag() (create\_dag.DAG method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{dag:create_dag.DAG.generate_dag}}\pysiglinewithargsret{\sphinxbfcode{generate\_dag}}{\emph{dag\_name}}{}
Create a python file for the DAG in the dags directory of AIRFLOW\_HOME. Generate the airflow code for
the dag in the file. The templates used in the function are taken in the {\hyperref[\detokenize{dag:module-create_dag}]{\sphinxcrossref{\sphinxcode{create\_dag}}}} module.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{dag\_name} \textendash{} the name of the dag to be generated

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
Currently it has the relative address of the folder as being contained in the myapp folder. Change it appropriately if you decide to
change the airflow home
\end{sphinxadmonition}

\end{fulllineitems}

\index{get\_drawable\_dag() (create\_dag.DAG method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{dag:create_dag.DAG.get_drawable_dag}}\pysiglinewithargsret{\sphinxbfcode{get\_drawable\_dag}}{\emph{G}, \emph{queries}, \emph{types}, \emph{edges}}{}
Helper function for {\hyperref[\detokenize{dag:create_dag.DAG.plot_dag}]{\sphinxcrossref{\sphinxcode{create\_dag.DAG.plot\_dag()}}}}. It gets the locations of the various figures
in the plotly plot of the DAG.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{queries} \textendash{} the queries in the DAG

\item {} 
\sphinxstyleliteralstrong{types} \textendash{} the types of queris. Not used, but can choose to get different colored rectangles for different query types

\item {} 
\sphinxstyleliteralstrong{edges} \textendash{} the edges in the DAG

\end{itemize}

\item[{Returns}] \leavevmode
a directed graph with the connections between inputs and outputs, the locations of bounding rectangles for the queries

\end{description}\end{quote}

\end{fulllineitems}

\index{plot\_dag() (create\_dag.DAG method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{dag:create_dag.DAG.plot_dag}}\pysiglinewithargsret{\sphinxbfcode{plot\_dag}}{}{}
Get the plotly div for the DAG. Get the locations using the helper function and then just plot those and return the html div for the plot
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
the html div of the DAG plotly plot

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\chapter{Generating alerts using Apache Flink}
\label{\detokenize{flink:generating-alerts-using-apache-flink}}\label{\detokenize{flink::doc}}
Tweets are continuously streamed from Twitter. Our system provides a functionality wherein it can detect certain user specified events in the live tweet stream. For example, user can make a specification to find viral hashtags in the sream. We leverage a couple of open-source technologies to detect user specified alerts in the Twitter stream. Apache Flink is an open-source, distributed and high performing stream processing tool. Apache Kafka, also open-source, is another streaming tool which can be used as a message queue for communication between programs in a highly available distributed fashion.

Tweets are continuously streamed from Twitter using the Twitter Streaming API and pushed to a Kafka topic (“tweets\_topic”) for downstream processes. This tweet stream is processed by Flink programs to detect the specified alerts (one Flink program per alert specification). Alert specifications are given by the user using an abstraction that we describe next.


\section{Alert Specification Abstraction}
\label{\detokenize{flink:alert-specification-abstraction}}
Our alert specification abstraction is inspired from Flink’s own specification. Each tweet is considered to have 4 attributes - UserId, Hashtags, URLs and User mentions. To specify an alert, user needs to specify the following:
\begin{itemize}
\item {} 
Filter - values of 0 or more tweet attributes to filter the tweets relevant for the alert.

\item {} 
Group keys - 0 or more of tweet attributes on which to group and split the tweet stream (1 sub-stream per group).

\item {} 
Window length (in seconds) - to divide each sub-stream into multiple windows, each of fixed length.

\item {} 
Window slide (in seconds) - to specify how often to start a new window (windows may overlap if slide \textless{} length).

\item {} 
Count - threshold of count of tweets in any window.

\end{itemize}

As soon as the count is reached in any window, an alert is generated.


\section{Back-end process}
\label{\detokenize{flink:back-end-process}}\begin{description}
\item[{The specification made by the user in our abstraction is processed as follows:}] \leavevmode\begin{itemize}
\item {} 
The specification is translated to a Flink Java application, compiled using Maven, uploaded and run by Flink server running locally.

\item {} 
This Flink Java application continuously streams the tweets from the Kafka topic (“tweets\_topic”), processes them according to the specification and posts any alerts on a different Kafka topic (“alerts\_topic”).

\item {} 
There is a simple Python application which continuously polls for alerts on this Kafka topic and persists any found alerts to MongoDB to be displayed by the dashboard.

\end{itemize}

\end{description}


\section{Example - Finding viral hashtags}
\label{\detokenize{flink:example-finding-viral-hashtags}}
Let us consider an example where we wish to be notified an alert if any hashtag is getting viral in the twitter stream. Suppose we define a hashtag as viral, if it is used more than 100 times in a span of 60 seconds. Now to describe this in our abstraction, we need to specify the following:
\begin{itemize}
\item {} 
Filter = None; as we need to consider all tweets.

\item {} 
Group keys = Hashtag; as we need to create a sub-stream for each hashtag.

\item {} 
Window length = 60

\item {} 
Window slide = 60; say, for non-overlapping windows

\item {} 
Count = 100

\end{itemize}


\section{Flink Code Generator Documentation}
\label{\detokenize{flink:flink-code-generator-documentation}}
Here we provide a documentation of the Flink code generator.
\phantomsection\label{\detokenize{flink:module-flink_code_gen}}\index{flink\_code\_gen (module)}
Module to generate Java code for Flink from the alert specification taken from user on the dashboard. You need to have jinja2 python module and Maven installed.
\index{FlinkCodeGenerator (class in flink\_code\_gen)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{flink:flink_code_gen.FlinkCodeGenerator}}\pysigline{\sphinxbfcode{class }\sphinxcode{flink\_code\_gen.}\sphinxbfcode{FlinkCodeGenerator}}
Bases: \sphinxcode{object}

Class to translate alert specification given by user to Java code for Flink. It uses a template defined in
flink\_template.txt. The user provided specification is translated to Java code and placed at appropriate
positions inside the template. This renders the complete Java file. The Java project is then created
containing this file and compiled using Maven.
\index{compile\_code() (flink\_code\_gen.FlinkCodeGenerator method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{flink:flink_code_gen.FlinkCodeGenerator.compile_code}}\pysiglinewithargsret{\sphinxbfcode{compile\_code}}{\emph{alert\_name}}{}
Compiles the java project for the given alert using maven and creates the jar file.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{alert\_name} \textendash{} The name of the alert whose code is to be compiled.

\item[{Returns}] \leavevmode
The path of the jar file resulting from the compilation.

\end{description}\end{quote}

\end{fulllineitems}

\index{delete\_code() (flink\_code\_gen.FlinkCodeGenerator method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{flink:flink_code_gen.FlinkCodeGenerator.delete_code}}\pysiglinewithargsret{\sphinxbfcode{delete\_code}}{\emph{alert\_name}}{}
Deletes the java project for the given alert name.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{alert\_name} \textendash{} The name of the alert whose code is to be deleted.

\end{description}\end{quote}

\end{fulllineitems}

\index{write\_code() (flink\_code\_gen.FlinkCodeGenerator method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{flink:flink_code_gen.FlinkCodeGenerator.write_code}}\pysiglinewithargsret{\sphinxbfcode{write\_code}}{\emph{alert\_name}, \emph{filter\_string}, \emph{group\_keys}, \emph{window\_length}, \emph{window\_slide}, \emph{threshold}}{}
Generates the java code for the given alert specification and writes tha java project for it in the alert’s base path.
Note: It can raise exception like alert already exists with the given name.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{alert\_name} \textendash{} Name of the alert to be created.

\item {} 
\sphinxstyleliteralstrong{filter\_string} \textendash{} Filter specification for the alert. Refer to \sphinxcode{flink\_code\_gen.FlinkCodeGenerator.\_get\_filter\_code()}.

\item {} 
\sphinxstyleliteralstrong{group\_keys} \textendash{} List of keys to group on. Refer to \sphinxcode{flink\_code\_gen.FlinkCodeGenerator.\_get\_duplication\_code()}.

\item {} 
\sphinxstyleliteralstrong{window\_length} \textendash{} Length of window in seconds. The threshold will be looked at each window in each sub-stream.

\item {} 
\sphinxstyleliteralstrong{window\_slide} \textendash{} Number of seconds after which to start each new window.

\item {} 
\sphinxstyleliteralstrong{threshold} \textendash{} Count threshold for tweets in each window to generate the alert.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{Flink API Documentation}
\label{\detokenize{flink:flink-api-documentation}}
Here we provide a documentation of the Flink API.
\phantomsection\label{\detokenize{flink:module-flink_api}}\index{flink\_api (module)}
Module to communicate with the Flink server running locally to do tasks like uploading an alert’s jar, running the jar as a Flink job,
cancelling a running job and checking the status of jobs.
\index{FlinkAPI (class in flink\_api)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{flink:flink_api.FlinkAPI}}\pysiglinewithargsret{\sphinxbfcode{class }\sphinxcode{flink\_api.}\sphinxbfcode{FlinkAPI}}{\emph{hostname='localhost'}, \emph{port=8081}}{}
Bases: \sphinxcode{object}

Class to communicate with Flink server.
\index{cancel\_job() (flink\_api.FlinkAPI method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{flink:flink_api.FlinkAPI.cancel_job}}\pysiglinewithargsret{\sphinxbfcode{cancel\_job}}{\emph{job\_id}}{}
Cancels the Flink job with the given job\_id.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{job\_id} \textendash{} job\_id of the Flink job to cancel.

\end{description}\end{quote}

\end{fulllineitems}

\index{check\_job\_status\_all() (flink\_api.FlinkAPI method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{flink:flink_api.FlinkAPI.check_job_status_all}}\pysiglinewithargsret{\sphinxbfcode{check\_job\_status\_all}}{}{}
Check the status of all jobs run in the past.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
Dictionary having key as alert\_name (which was supplied in run\_jar) and value as status of the last job of that alert.

\end{description}\end{quote}

\end{fulllineitems}

\index{run\_jar() (flink\_api.FlinkAPI method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{flink:flink_api.FlinkAPI.run_jar}}\pysiglinewithargsret{\sphinxbfcode{run\_jar}}{\emph{alert\_name}, \emph{flink\_jar\_id}}{}
Runs the given jar\_id on Flink.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{alert\_name} \textendash{} Name of the alert to which the jar\_id belongs.

\item {} 
\sphinxstyleliteralstrong{flink\_jar\_id} \textendash{} jar\_id to be run on Flink.

\end{itemize}

\item[{Returns}] \leavevmode
The job\_id of the Flink job started as returned by the Flink server.

\end{description}\end{quote}

\end{fulllineitems}

\index{upload\_jar() (flink\_api.FlinkAPI method)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{flink:flink_api.FlinkAPI.upload_jar}}\pysiglinewithargsret{\sphinxbfcode{upload\_jar}}{\emph{jar\_path}}{}
Uploads the jar of the alert to the Flink server running locally.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{jar\_path} \textendash{} Path of the jar file to upload.

\item[{Returns}] \leavevmode
The jar\_id as returned by Flink server.

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\section{Flink Alerts Consumer}
\label{\detokenize{flink:flink-alerts-consumer}}
Here we provide a documentation of the Kafka consumer (for the topic ‘alerts\_topic”) for Flink alerts.
\phantomsection\label{\detokenize{flink:module-kafka_flink_alerts_consumer}}\index{kafka\_flink\_alerts\_consumer (module)}
Module to read alerts from the Kafka topic (“alerts\_topic”) where Flink applications post alerts and then put them in MongoDB.
\index{insert\_records() (in module kafka\_flink\_alerts\_consumer)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{flink:kafka_flink_alerts_consumer.insert_records}}\pysiglinewithargsret{\sphinxcode{kafka\_flink\_alerts\_consumer.}\sphinxbfcode{insert\_records}}{\emph{records}}{}
\end{fulllineitems}



\chapter{Benchmarking the query answering}
\label{\detokenize{benchmarking:benchmarking-the-query-answering}}\label{\detokenize{benchmarking::doc}}

\section{Neo4j queries}
\label{\detokenize{benchmarking:neo4j-queries}}
We divide the number queries to be answered into two types. Simple  neo4j queries and complex neo4j queries. We have these queries and a set list of hashtags and userIds present in the system. We generate a list of queries to be answered by randomly picking attribute to create the query. Thus creating a single query consists of these two steps:
\begin{itemize}
\item {} 
Pick a templated cypher query

\item {} 
Randomly pick the values of inputs to the query from a static list to create the query.

\end{itemize}

Similarly, we create a list of queries. The templated cypher queries are put into the simple or complex basket by seeing the time taken to answer a single query.

Having obtained the queries, we spawn multiple threads each of which opens a connection to the neo4j database in form of a session. Pops a query from the queue and delegates it to be answered by the database. To observe the optimal number of connections to be opened to the database, we plot the query answering rate verses the number of parallel connections.


\subsection{Simple Queries}
\label{\detokenize{benchmarking:simple-queries}}
The simple queries considered are these:
\begin{itemize}
\item {} 
Return count of distinct users who have used a hashtag

\item {} 
Return count of ditinct users who follow a certain user

\item {} 
Return the number of times a user was followed in a given interval

\item {} 
Find the number of current followers of users who have used certain

\item {} 
Find the count of users who tweeted in a given interval

\end{itemize}

The cypher code of these queries can also be seen here.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Return count of distinct users who have used a hashtag}
\PYG{n}{q1} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{l+s+s2}{match (u:USER)\PYGZhy{}[:TWEETED]\PYGZhy{}\PYGZgt{}(t:TWEET)\PYGZhy{}[:HAS\PYGZus{}HASHTAG]\PYGZhy{}\PYGZgt{}(:HASHTAG\PYGZob{}text:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZob{}\PYGZob{}h\PYGZcb{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZcb{})}
\PYG{l+s+s2}{with distinct u as u1 return count(u1)}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{c+c1}{\PYGZsh{} Return count of ditinct users who follow a certain user}
\PYG{n}{q2} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}\PYG{l+s+s2}{match (x:USER)\PYGZhy{}[:FOLLOWS]\PYGZhy{}\PYGZgt{}(:USER \PYGZob{}id:\PYGZob{}\PYGZob{}u1\PYGZcb{}\PYGZcb{}\PYGZcb{}), (x)\PYGZhy{}[:FOLLOWS]\PYGZhy{}\PYGZgt{}(:USER \PYGZob{}id:\PYGZob{}\PYGZob{}u2\PYGZcb{}\PYGZcb{}\PYGZcb{})}
\PYG{l+s+s2}{with distinct x as x1 return count(x1)}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{c+c1}{\PYGZsh{} Return the number of times a user was followed in a given interval}
\PYG{n}{q3} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+s2}{match (fe:FOLLOW\PYGZus{}EVENT)\PYGZhy{}[:FE\PYGZus{}FOLLOWED]\PYGZhy{}\PYGZgt{}(u:USER \PYGZob{}id:\PYGZob{}\PYGZob{}u\PYGZcb{}\PYGZcb{}\PYGZcb{})}
\PYG{l+s+s2}{where fe.timestamp \PYGZgt{} \PYGZob{}\PYGZob{}t1\PYGZcb{}\PYGZcb{} and fe.timestamp \PYGZlt{} \PYGZob{}\PYGZob{}t2\PYGZcb{}\PYGZcb{}}
\PYG{l+s+s2}{return count(fe)}
\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{c+c1}{\PYGZsh{} Find the number of current followers of users who have used certain hahstag}
\PYG{n}{q4} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+s2}{match (x:USER \PYGZob{}id:\PYGZob{}\PYGZob{}u\PYGZcb{}\PYGZcb{}\PYGZcb{})\PYGZhy{}[:TWEETED]\PYGZhy{}\PYGZgt{}(:TWEET)\PYGZhy{}[:HAS\PYGZus{}HASHTAG]\PYGZhy{}\PYGZgt{}(h:HASHTAG), (f:USER)\PYGZhy{}[:FOLLOWS]\PYGZhy{}\PYGZgt{}(x), (f)\PYGZhy{}[:TWEETED]\PYGZhy{}\PYGZgt{}(:TWEET)\PYGZhy{}[:HAS\PYGZus{}HASHTAG]\PYGZhy{}\PYGZgt{}(h)}
\PYG{l+s+s2}{with distinct f as f1 return count(f1)}
\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{c+c1}{\PYGZsh{} Find the count of users who tweeted in a given interval}
\PYG{n}{q5} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+s2}{match (te:TWEET\PYGZus{}EVENT)\PYGZhy{}[:TE\PYGZus{}TWEET]\PYGZhy{}\PYGZgt{}(:TWEET)\PYGZhy{}[:RETWEET\PYGZus{}OF]\PYGZhy{}\PYGZgt{}(t:TWEET), (te)\PYGZhy{}[:TE\PYGZus{}USER]\PYGZhy{}\PYGZgt{}(:USER \PYGZob{}id:\PYGZob{}\PYGZob{}u\PYGZcb{}\PYGZcb{}\PYGZcb{}), (x:USER)\PYGZhy{}[:TWEETED]\PYGZhy{}\PYGZgt{}(t)}
\PYG{l+s+s2}{where te.timestamp \PYGZlt{} \PYGZob{}\PYGZob{}t1\PYGZcb{}\PYGZcb{} and te.timestamp \PYGZgt{} \PYGZob{}\PYGZob{}t2\PYGZcb{}\PYGZcb{}}
\PYG{l+s+s2}{with distinct x as x1 return count(x1)}
\end{sphinxVerbatim}

Using these templated queries we generate the list of simple queries to be fired as described above and observe the query ansering rate with number of parallel sessions. This graph is obtained:

\noindent\sphinxincludegraphics{{simple}.png}

As we obtain the best query rate is obtained on opening 4 parallel sessions of about 1100 queries/second in answering of simple queries.


\subsection{Complex Queries}
\label{\detokenize{benchmarking:complex-queries}}
For complex queries we try to consider those queries which require more than one hop in the network. The complex queries considered are these:
\begin{itemize}
\item {} 
Find common followers of two users

\item {} 
Return the users which follow a user u and tweeted t(which mentions same u) b/w t1 and t2

\item {} 
Find users which have tweeted tweet t1(retweet of another tweet t containing hashtag hash AND such that t1 itself contains the same hashtag hash) b/w time1 and time2 and follows u

\item {} 
Find users which follow user with id u1 and follow user u which tweeted b/w t1 and t2 containing hashtag hash

\end{itemize}

The cypher code of these queries is as follows.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Find common followers of two users}
\PYG{n}{q6} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+s2}{MATCH (u1 :USER \PYGZob{}id:\PYGZob{}\PYGZob{}id1\PYGZcb{}\PYGZcb{}\PYGZcb{}),(u2 :USER \PYGZob{}id:\PYGZob{}\PYGZob{}id2\PYGZcb{}\PYGZcb{}\PYGZcb{}), (user :USER)}
\PYG{l+s+s2}{WHERE (u1) \PYGZlt{}\PYGZhy{}[:FOLLOWS]\PYGZhy{} (user) AND (user) \PYGZhy{}[:FOLLOWS]\PYGZhy{}\PYGZgt{} (u2)}
\PYG{l+s+s2}{RETURN count(user)}
\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{c+c1}{\PYGZsh{} Find the users which follow a user u and tweeted t(which mentions same u) b/w t1 and t2}
\PYG{n}{q7}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+s2}{MATCH (run:RUN) \PYGZhy{}[:HAS\PYGZus{}FRAME]\PYGZhy{}\PYGZgt{} (frame1:FRAME)}
\PYG{l+s+s2}{WHERE  frame1.end\PYGZus{}t \PYGZgt{}= \PYGZob{}\PYGZob{}t1\PYGZcb{}\PYGZcb{} AND frame1.start\PYGZus{}t \PYGZlt{}= \PYGZob{}\PYGZob{}t2\PYGZcb{}\PYGZcb{}}
\PYG{l+s+s2}{MATCH (frame1) \PYGZhy{}[:HAS\PYGZus{}TWEET]\PYGZhy{}\PYGZgt{} (event1 :TWEET\PYGZus{}EVENT), (event1) \PYGZhy{}[:TE\PYGZus{}USER]\PYGZhy{}\PYGZgt{} (x :USER \PYGZob{}\PYGZcb{}), (event1) \PYGZhy{}[:TE\PYGZus{}TWEET]\PYGZhy{}\PYGZgt{} (t :TWEET \PYGZob{}\PYGZcb{}), (x :USER \PYGZob{}\PYGZcb{}) \PYGZhy{}[:FOLLOWS]\PYGZhy{}\PYGZgt{} (u :USER \PYGZob{}id:\PYGZob{}\PYGZob{}u\PYGZcb{}\PYGZcb{}\PYGZcb{}), (t) \PYGZhy{}[:HAS\PYGZus{}MENTION]\PYGZhy{}\PYGZgt{} (u :USER \PYGZob{}id:\PYGZob{}\PYGZob{}u\PYGZcb{}\PYGZcb{}\PYGZcb{})}
\PYG{l+s+s2}{RETURN  COUNT(x)}
\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{c+c1}{\PYGZsh{} Find users which have tweeted tweet t1(retweet of another tweet t containing hashtag hash AND such that t1 itself contains the same hashtag hash) b/w time1 and time2 and follows u}
\PYG{n}{q8} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+s2}{MATCH (run:RUN) \PYGZhy{}[:HAS\PYGZus{}FRAME]\PYGZhy{}\PYGZgt{} (frame1:FRAME)}
\PYG{l+s+s2}{WHERE  frame1.end\PYGZus{}t \PYGZgt{}= \PYGZob{}\PYGZob{}time1\PYGZcb{}\PYGZcb{} AND frame1.start\PYGZus{}t \PYGZlt{}= \PYGZob{}\PYGZob{}time2\PYGZcb{}\PYGZcb{}}
\PYG{l+s+s2}{MATCH (frame1) \PYGZhy{}[:HAS\PYGZus{}TWEET]\PYGZhy{}\PYGZgt{} (event1 :TWEET\PYGZus{}EVENT), (event1) \PYGZhy{}[:TE\PYGZus{}USER]\PYGZhy{}\PYGZgt{} (x :USER \PYGZob{}\PYGZcb{}), (event1) \PYGZhy{}[:TE\PYGZus{}TWEET]\PYGZhy{}\PYGZgt{} (t1 :TWEET \PYGZob{}\PYGZcb{}), (x :USER \PYGZob{}\PYGZcb{}) \PYGZhy{}[:FOLLOWS]\PYGZhy{}\PYGZgt{} (u :USER \PYGZob{}id:\PYGZob{}\PYGZob{}u\PYGZcb{}\PYGZcb{}\PYGZcb{}), (t :TWEET \PYGZob{}\PYGZcb{}) \PYGZhy{}[:HAS\PYGZus{}HASHTAG]\PYGZhy{}\PYGZgt{} (:HASHTAG \PYGZob{}text:}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{\PYGZob{}\PYGZob{}hash\PYGZcb{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{\PYGZcb{}), (t1 :TWEET \PYGZob{}\PYGZcb{}) \PYGZhy{}[:HAS\PYGZus{}HASHTAG]\PYGZhy{}\PYGZgt{} (:HASHTAG \PYGZob{}text:}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{\PYGZob{}\PYGZob{}hash\PYGZcb{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{\PYGZcb{}), (t1) \PYGZhy{}[:RETWEET\PYGZus{}OF]\PYGZhy{}\PYGZgt{} (t)}
\PYG{l+s+s2}{RETURN  COUNT(x)}
\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}

\PYG{c+c1}{\PYGZsh{} Find users which follow user with id u1 and follow user u which tweeted b/w t1 and t2 containing hashtag hash}
\PYG{n}{q9} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+s2}{MATCH (run:RUN) \PYGZhy{}[:HAS\PYGZus{}FRAME]\PYGZhy{}\PYGZgt{} (frame1:FRAME)}
\PYG{l+s+s2}{WHERE  frame1.end\PYGZus{}t \PYGZgt{}= \PYGZob{}\PYGZob{}t1\PYGZcb{}\PYGZcb{} AND frame1.start\PYGZus{}t \PYGZlt{}= \PYGZob{}\PYGZob{}t2\PYGZcb{}\PYGZcb{}}
\PYG{l+s+s2}{MATCH (frame1) \PYGZhy{}[:HAS\PYGZus{}TWEET]\PYGZhy{}\PYGZgt{} (event1 :TWEET\PYGZus{}EVENT), (event1) \PYGZhy{}[:TE\PYGZus{}USER]\PYGZhy{}\PYGZgt{} (u :USER \PYGZob{}\PYGZcb{}), (event1) \PYGZhy{}[:TE\PYGZus{}TWEET]\PYGZhy{}\PYGZgt{} (t :TWEET \PYGZob{}\PYGZcb{}), (x :USER \PYGZob{}\PYGZcb{}) \PYGZhy{}[:FOLLOWS]\PYGZhy{}\PYGZgt{} (u1 :USER \PYGZob{}id:\PYGZob{}\PYGZob{}u1\PYGZcb{}\PYGZcb{}\PYGZcb{}), (x) \PYGZhy{}[:FOLLOWS]\PYGZhy{}\PYGZgt{} (u), (t :TWEET \PYGZob{}\PYGZcb{}) \PYGZhy{}[:HAS\PYGZus{}HASHTAG]\PYGZhy{}\PYGZgt{} (:HASHTAG \PYGZob{}text:}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{\PYGZob{}\PYGZob{}hash\PYGZcb{}\PYGZcb{}}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{\PYGZcb{})}
\PYG{l+s+s2}{RETURN  COUNT(x)}
\PYG{l+s+s2}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\end{sphinxVerbatim}

Using these templated queries we generate the list of complex queries to be fired as described above and observe the query answering rate with number of parallel sessions. The following graph is obtained:

\noindent\sphinxincludegraphics{{complex}.png}

As we obtain the best query rate is obtained on opening 10 parallel sessions of about 65 queries/second in answering of complex queries.

Further observe that the peak performance in query answering is obtained at lesser number of parallel connections as compared to the compex case. This is because there is overhead in maintaining parallel connections in neo4j, which involves maintaining the sesions and delegating the queries to the database. This overhead is much more prominent in case when the queries itself take much less time in answering them as compared to the complex case where the overhead gets ammortised better.


\section{MongoDB queries}
\label{\detokenize{benchmarking:mongodb-queries}}
MongoDB queries are generally answered very fast in comparison to the neo4j queries, which is owing to the intended schemas of the two databases. Thus, no further observations were made in mongoDB query answering apart from observing that all the queries are answered in mili second scale.


\section{Code Documentation for benchmarking}
\label{\detokenize{benchmarking:code-documentation-for-benchmarking}}
Here we provide a documentation of the code used in this sections functionality.
\phantomsection\label{\detokenize{benchmarking:module-query_answering}}\index{query\_answering (module)}
Module to benchmark the query answering rate for neo4j.
\begin{itemize}
\item {} 
Generate a list of queries.

\item {} 
Open multple neo4j sessions.

\item {} 
See what is the peak rate.

\end{itemize}
\index{answer\_queries() (in module query\_answering)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{benchmarking:query_answering.answer_queries}}\pysiglinewithargsret{\sphinxcode{query\_answering.}\sphinxbfcode{answer\_queries}}{\emph{query\_l}}{}
Function to answer all queries from a list of queries in sequential manner
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{query\_l} \textendash{} the list of cypher queries

\end{description}\end{quote}

\end{fulllineitems}

\index{answer\_queries\_par() (in module query\_answering)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{benchmarking:query_answering.answer_queries_par}}\pysiglinewithargsret{\sphinxcode{query\_answering.}\sphinxbfcode{answer\_queries\_par}}{\emph{query\_l}, \emph{num\_procs}}{}
Function to answer all queries from a list of queries in concurrent manner. Spawn \textless{}num\_procs\textgreater{} number
of processes. Each process opens a session and executes a cypher query.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{query\_l} \textendash{} the list of cypher queries

\item {} 
\sphinxstyleliteralstrong{num\_procs} \textendash{} number of processes to create

\end{itemize}

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
There will only be atmost k number of real parallel process in the system, where k is number of cores. This
number is further limited by the session management of neo4j, which is what we observe in the profile difference between simple and complex queries.
\end{sphinxadmonition}

\end{fulllineitems}

\index{answer\_query() (in module query\_answering)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{benchmarking:query_answering.answer_query}}\pysiglinewithargsret{\sphinxcode{query\_answering.}\sphinxbfcode{answer\_query}}{\emph{query}}{}
Answer a single cypher query \textless{}query\textgreater{}
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{query} \textendash{} cypher query to be answered

\end{description}\end{quote}

\end{fulllineitems}

\index{generate\_random\_queries() (in module query\_answering)}

\begin{fulllineitems}
\phantomsection\label{\detokenize{benchmarking:query_answering.generate_random_queries}}\pysiglinewithargsret{\sphinxcode{query\_answering.}\sphinxbfcode{generate\_random\_queries}}{\emph{total}}{}
Generate a list containing \textless{}total\textgreater{} cypher queries. Consider templated cypher queries and lists of attributes. Randomly put in the attributs into the cypher
query tempate to get an executable query.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{total} \textendash{} number of queris to generate

\end{description}\end{quote}

\begin{sphinxadmonition}{note}{Note:}
Obiviously, one need to change the list of attibutes accordingly, if they choose to benchmark on a different dataset.
\end{sphinxadmonition}

\end{fulllineitems}



\chapter{Dashboard Website}
\label{\detokenize{dashboard_website:dashboard-website}}\label{\detokenize{dashboard_website::doc}}

\section{Major parts of the dashboard website}
\label{\detokenize{dashboard_website:major-parts-of-the-dashboard-website}}
We have organised the dashboard website into tabs with each tab containing associated APIs and functionality. Each tab is futher divided into sub tabs. We enlist the major tabs and subtabs and the functionality contained there in, to give an overview of the hierarchy of the website.


\subsection{Hashtags}
\label{\detokenize{dashboard_website:hashtags}}
This tab contains the functionality to view major statistics associated with hashtags. Though as we will see, the functionality in this tab can entirely be emulated by creating a suitable DAG, but we choose to keep a separate option to get some common stats about the common entities like hastags, user mentions and urls.

The Hashtags tab contains three subtabs:
\begin{itemize}
\item {} 
Top 10 : Takes as inputs the start time and the end time, to output the 10 most popular hashtags in the said interval with the number of tweets containing the hashtag

\item {} 
Usage Plot : Takes as input a hashtag, start time and end time, to output the plot of how the usage(as number of tweets in which the hashtag occurs) of the hashtag has changed over the interval.

\item {} 
Sentiment Plot : Takes as input a hashtag, start time and end time, to output the plot of how the sentiment associated with the hashtag has changed over the interval.

\end{itemize}


\subsection{Mentions}
\label{\detokenize{dashboard_website:mentions}}
The Mentions tab contains the major statistics concerning user mentions. It has the follwoing three subtabs:
\begin{itemize}
\item {} 
Top 10 : Takes as inputs the start time and the end time, to output the 10 most popular users in the said interval with the number of tweets in which the user is mentioned.

\item {} 
Usage Plot : Takes as input a user, start time and end time, to output the plot of how the mention frequency(as number of tweets in which the user is mentioned) of the user has changed over the interval.

\item {} 
Sentiment Plot : Takes as input a user, start time and end time, to output the plot of how the sentiment associated with the user has changed over the interval.

\end{itemize}


\subsection{URLs}
\label{\detokenize{dashboard_website:urls}}
The URLs tab contains the major statistics concerning urls. It has the follwoing three subtabs:
\begin{itemize}
\item {} 
Top 10 : Takes as inputs the start time and the end time, to output the 10 most popular urls in the said interval with the number of tweets containing the url.

\item {} 
Usage Plot : Takes as input a url, start time and end time, to output the plot of how the usage(as number of tweets in which the url occurs) of the url has changed over the interval.

\item {} 
Sentiment Plot : Takes as input a url, start time and end time, to output the plot of how the sentiment associated with the url has changed over the interval.

\end{itemize}


\subsection{Alerts}
\label{\detokenize{dashboard_website:alerts}}

\subsection{DAG}
\label{\detokenize{dashboard_website:dag}}
We expalin about DAGs in detail in section {\hyperref[\detokenize{dag:composing-multiple-queries-dag}]{\sphinxcrossref{\DUrole{std,std-ref}{Composing multiple queries : DAG}}}}. We assume the reader has read though the section and is aware with the terminology.

This tab contains the functionalities to create and view DAGs. It has the following subtabs:
\begin{itemize}
\item {} 
Create Neo4j Query : Contains the APIs to create a neo4j queries. The user provides the inputs for query cration thorugh a simple form.

\item {} 
Create MongoDB Query : Contains the APIs to create mongoDB queries.

\item {} 
Create Post-Processing Function: Contains the APIs to create a post processing function. The form contains a file upload field thorugh which the file containing the python code for the function needs to be uploaded.

\item {} 
All Queries : A color coded list of all queries created by the user, along with their input and output variables names. The user can delete queries from here.

\item {} 
Create DAG : Compose the queries seen in the list of queries to create a DAG. The structure need to be specified in a file which needs to be uploaded.

\item {} 
View DAG : Contains a list of DAGs created by the user. Also contains a button through which the user can go the airflow dashboard. Apart from that, with each DAG there is a “View” button which redirects to a page containg the strucutre code and the plotly graph of the DAG.

\item {} 
Create Custom Metric : Contains a form in which the user needs to specify a DAG and a post processing function to create metric and view it either in plot/graph format.

\end{itemize}


\section{Use Cases}
\label{\detokenize{dashboard_website:use-cases}}
Here we walk through some major use cases of the system with snapshots to give the reader some headway on how to use the system. The system has been designed, keeping in mind that the end user may not be much proficient in computer technology and has been strucutured to be intuitive and simple. Nonetheless, the authors feel that these use case should be enough to get the user started.

Also, please notice that the easlier use cases may be used in the later ones. So it’s better the reader goes through these in order.


\subsection{Viewing top 10 popular hashtags}
\label{\detokenize{dashboard_website:viewing-top-10-popular-hashtags}}
Go to the Hashtags/Top 10, enter the required fields. The list of top 10 most popular hahstags will be displayed below.

\sphinxincludegraphics[scale=0.25]{{hash1}.png}  \sphinxincludegraphics[scale=0.25]{{hash2}.png}

Lets take a hashtag and view its statsitics in below couple of use cases.


\subsection{Viewing usage history of  hashtag}
\label{\detokenize{dashboard_website:viewing-usage-history-of-hashtag}}
Let’s see how the usage of hashtag “GOT7” has changed over a period of 2 days.

\noindent\sphinxincludegraphics[scale=0.4]{{hash3}.png}


\subsection{Viewing sentiment history of  hashtag}
\label{\detokenize{dashboard_website:viewing-sentiment-history-of-hashtag}}
Let’s see how the sentiment about hashtag “GOT7” has changed over the same period of 2 days.

\noindent\sphinxincludegraphics[scale=0.4]{{hash4}.png}


\subsection{Creating a mongoDB query}
\label{\detokenize{dashboard_website:creating-a-mongodb-query}}\begin{quote}

Let’s create a mongo DB query named “most\_popular\_hashtags\_20” to give us the 20 most popular hashtags. Specify the variables and click “create” to create the query.

\noindent\sphinxincludegraphics[scale=0.4]{{mongo1}.png}
\end{quote}

Similarly other mongoDB queries can be created.


\subsection{Creating neo4j queries}
\label{\detokenize{dashboard_website:creating-neo4j-queries}}
To crate a neo4j query we need to create user and tweet entities and relationships between them. Here we show how to create the neo4j to get userIds and their tweet counts who have used one of the hashtags from a list of hashtags atleast once and have tweeted with one of the popular user mentions from a list of userIds atleast once, mentioned in {\hyperref[\detokenize{dag:building-a-dag-from-queries}]{\sphinxcrossref{\DUrole{std,std-ref}{Building a DAG from queries}}}}.

Create a user variable named user with no attributes. Also create a user variable named user\_mentioned having variable attribute \{um\_id\}. The curly braces specify that the attribute is variable.
\begin{quote}

\noindent\sphinxincludegraphics[scale=0.4]{{neo1}.png}
\end{quote}

Let us now create some tweets. Create a tweet named t1 having variable hashtag \{hashtag\}. Create a tweet t2 which has a mention of user User\_mentioned, which was created above. Also, create a tweet t3 having no attributes.
\begin{quote}

\noindent\sphinxincludegraphics[scale=0.4]{{neo2}.png}
\end{quote}

Lets now create some relation ships. Create the relationships, user tweeted tweet t1, user tweeted tweet t2 and user tweeted t3.
\begin{quote}

\noindent\sphinxincludegraphics[scale=0.4]{{neo3}.png}
\end{quote}

So finally we have 2 user variables, 3 tweet variables and 3 relationships between the entities. This can be seen in this image where a screenshot of the tweets and relationships listing is shown.
\begin{quote}

\noindent\sphinxincludegraphics[scale=0.4]{{neo4}.png}
\end{quote}

To create the query specify the return variables and the query name.
\begin{quote}

\noindent\sphinxincludegraphics[scale=0.4]{{neo5}.png}
\end{quote}


\subsection{Create Post processing function}
\label{\detokenize{dashboard_website:create-post-processing-function}}
Select a file containing the python code to create a post processing function
\begin{quote}

\noindent\sphinxincludegraphics[scale=0.4]{{pp1}.png}
\end{quote}


\subsection{View Queries}
\label{\detokenize{dashboard_website:view-queries}}
To view the queries navigate to DAG/All Queries. As you can see here, currently we have 4 queries, 2 mongo DB and 1 neo4j and 1 post processing function. Additionally, you can see the cypher code generated for the neo4j query and the code of the post processing function in this screenshot:
\begin{quote}

\noindent\sphinxincludegraphics[scale=0.4]{{queries2}.png}
\end{quote}


\subsection{Create DAG}
\label{\detokenize{dashboard_website:create-dag}}
Now let us create the DAG to get the most active users as mentioned in {\hyperref[\detokenize{dag:building-a-dag-from-queries}]{\sphinxcrossref{\DUrole{std,std-ref}{Building a DAG from queries}}}}.
Input the name of the DAG as “activer\_users\_dag”, optionally the description and the file containing the structure specification of the DAG.
\begin{quote}

\noindent\sphinxincludegraphics[scale=0.4]{{dag1}.png}
\end{quote}


\subsection{View DAGs}
\label{\detokenize{dashboard_website:view-dags}}
Navigate to the View DAG subtab to view all the created DAGs.

\noindent\sphinxincludegraphics[scale=0.4]{{dag2}.png}

We can view a DAG by clicking “View” button against it. You can see how outputs from one query are feeding into the inputs of another query. Beneath in the screenshot you can see the structure of the DAG in code as well.

\noindent\sphinxincludegraphics[scale=0.4]{{dag3}.png}

Now let us view our DAG in airflow. Here you can see the tree view and the graph view of the DAG.

\sphinxincludegraphics[scale=0.25]{{dag5}.png}  \sphinxincludegraphics[scale=0.25]{{dag6}.png}

Execute the DAG in airflow and navigate to XComs list to see the outputs of all the queries. A screensot of the XComs list is provided here.

\noindent\sphinxincludegraphics[scale=0.4]{{dag7}.png}


\subsection{Create Custom metric}
\label{\detokenize{dashboard_website:create-custom-metric}}
To create the custom metric, we need to specify the DAG which we want to execute, choose a post processing function which outputs the x and y coordinates and create a mapping between the outputs of the DAG and inputs of the post processing function. Shown here is how to create a custom metric on the most active users DAG to plot the 10 top active user Ids with their number of tweets:

\noindent\sphinxincludegraphics[scale=0.4]{{cm1}.png}

When you click Fetch data the DAG will be executed to feed data into the post processing function. You can now view in a plot or table format by clicking on “PLOT GRAPH!” and “CREATE TABLE!” respectively. The table will look something like this:

\noindent\sphinxincludegraphics[scale=0.4]{{cm2}.png}


\subsection{Create Alert}
\label{\detokenize{dashboard_website:create-alert}}
To create an alert on the tweet stream, we need to specify the alert name, the filter, choose keys on which to group and partition the tweet stream, the window length, the window slide and the count threshold. Let’s create a hashtag “viral\_hashtags” to notify when a hashtag frequency exceeds 3 in the past window of 60 seconds, the window sliding ahead by 30 seconds.

\noindent\sphinxincludegraphics[scale=0.4]{{alerts1}.png}


\subsection{View Alerts}
\label{\detokenize{dashboard_website:view-alerts}}
The alerts are generated as real time tweets are put into the kafka queue.

\noindent\sphinxincludegraphics[scale=0.4]{{alerts2}.png}

In the end, the best way to figure out the system is to get your hands dirty with the system! To get the system on your local system, the reader should see {\hyperref[\detokenize{running:getting-the-system-running}]{\sphinxcrossref{\DUrole{std,std-ref}{Getting the system running}}}}


\chapter{Getting the system running}
\label{\detokenize{running:getting-the-system-running}}\label{\detokenize{running::doc}}

\section{Setting up the environment}
\label{\detokenize{running:setting-up-the-environment}}
We recommend getting conda(or miniconda if you are low on disk space). Installing conda is easy, and the installation instructions can be found on the \sphinxhref{https://conda.io/docs/user-guide/install/download.html}{download page} itself. After installation of conda, run the following commands:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
// create a new virtual environment
conda create \PYGZhy{}\PYGZhy{}name twitter\PYGZus{}analytics \PYG{n+nv}{python}\PYG{o}{=}\PYG{o}{=}\PYG{l+m}{3}.6 \PYGZhy{}\PYGZhy{}file requirements\PYGZus{}conda.txt
// clone the repo
git clone https://github.com/abhi19gupta/TwitterAnalytics.git
// \PYG{n+nb}{cd} into the repo
\PYG{n+nb}{cd} TwitterAnalytics
// activate the virtual envt
\PYG{n+nb}{source} activate twitter\PYGZus{}analytics
// install the required modules.
pip install \PYGZhy{}r requirements\PYGZus{}pip.txt
.
.
// deactivate the virtual environment
\PYG{n+nb}{source} deactivate
\end{sphinxVerbatim}

Please note that the requirements mentioned above are a superset of system’s actual requirements. It may contain some ML related modules which were used while trying NER on tweets, but are not used in the final system.

The entire system has been designed and tested on Ubuntu 16.04 machine. It may work on Windows with appropriate tweaks.

Apart from these, the user also needs to install the following:
\begin{itemize}
\item {} 
MongoDB: Follow the installation instructions on this \sphinxhref{https://docs.mongodb.com/manual/tutorial/install-mongodb-on-ubuntu/}{MongoDB installation link}.

\item {} 
Neo4j: Follow the installation instructions on this \sphinxhref{https://neo4j.com/docs/operations-manual/current/installation/linux/debian/}{Neo4j installation link}.

\item {} 
Apache Flink: Download the Flink 1.4.2 binary from this \sphinxhref{https://archive.apache.org/dist/flink/flink-1.4.2/}{Flink binary link}. If this does not work, download the latest version from the \sphinxhref{https://flink.apache.org/downloads.html}{Flink download page}. Once downloaded, simply extract the zip and the installation is complete. You need to use the path of this installation in the shell script as mentioned in {\hyperref[\detokenize{running:running-flink-and-kafka}]{\sphinxcrossref{\DUrole{std,std-ref}{Running Flink and Kafka}}}}.

\item {} 
Apache Kafka: Download the Kafka 2.11 binary from thie \sphinxhref{https://www.apache.org/dyn/closer.cgi?path=/kafka/1.1.0/kafka\_2.11-1.1.0.tgz}{Kafka binary link}. If this does not work, download the latest version from the \sphinxhref{https://kafka.apache.org/downloads}{Kafka download page}. Once downloaded, simply extract the zip and the installation is complete. You need to use the path of this installation in the shell script as mentioned in {\hyperref[\detokenize{running:running-flink-and-kafka}]{\sphinxcrossref{\DUrole{std,std-ref}{Running Flink and Kafka}}}}.

\item {} 
Apache Airflow: Follow the installation instructions on this \sphinxhref{https://airflow.apache.org/installation.html}{Airflow installation link}.

\end{itemize}


\section{Running the system}
\label{\detokenize{running:running-the-system}}
To set the system up, the following steps need to be taken.


\subsection{Collecting data}
\label{\detokenize{running:collecting-data}}
Navigate to ‘Read Twitter Stream’ and run \sphinxcode{python streaming.py} to collect streaming data or run \sphinxcode{python userstimeline.py} to collect data for a set of users.

More details can be found here: {\hyperref[\detokenize{twitter_stream:read-data-from-twitter-api}]{\sphinxcrossref{\DUrole{std,std-ref}{Read data from Twitter API}}}}.


\subsection{Ingesting data}
\label{\detokenize{running:ingesting-data}}
Before proceeding, start the Neo4j and MongoDB servers locally (using service commands in Ubuntu. Eg. \sphinxcode{service neo4j start}, \sphinxcode{service mongod start}).

Once the data is collected, you can ingest it into the databases as follows:
\begin{itemize}
\item {} 
Neo4j: Navigate to Ingestion/Neo4j/ and run \sphinxcode{python ingest\_neo4j\_streaming.py} to ingest data collected using streaming API or run \sphinxcode{python ingest\_neo4j\_user\_timeline.py} to ingest data collected using User Timeline API.

\item {} 
MongDB: In MongoDB we provide functionality to store only streaming data. Navigate to Ingestion/MongoDB and run \sphinxcode{python ingest\_raw.py}.

\end{itemize}

More details can be found here: {\hyperref[\detokenize{mongoDB_data_ingestion:ingesting-data-into-mongodb}]{\sphinxcrossref{\DUrole{std,std-ref}{Ingesting data into MongoDB}}}} and {\hyperref[\detokenize{neo4j_data_ingestion:ingesting-data-into-neo4j}]{\sphinxcrossref{\DUrole{std,std-ref}{Ingesting data into Neo4j}}}}


\subsection{Running the dashboard}
\label{\detokenize{running:running-the-dashboard}}
To run the website on a local server on your machine, navigate to Dashboard\_Website/ and run \sphinxcode{python manage.py runserver}.


\subsection{Running Flink and Kafka}
\label{\detokenize{running:running-flink-and-kafka}}
To test the Flink applications generated on the dashboard, you need to do the following:
\begin{itemize}
\item {} 
Start the Kafka and Flink servers locally using the shell script in ‘Kafka/’ folder. (Configure the path of the Kafka and Flink directories in the shell script).

\item {} 
You can now create alert specifications on the dashboard.

\item {} 
Now, to put some tweets on the Kafka topic ‘tweets\_topic’. Navigate to ‘Kafka/’ and run \sphinxcode{python kafka\_tweets\_producer.py} after configuring the data directory in its main function.

\end{itemize}


\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{c}
\item {\sphinxstyleindexentry{create\_dag}}\sphinxstyleindexpageref{dag:\detokenize{module-create_dag}}
\indexspace
\bigletter{e}
\item {\sphinxstyleindexentry{execute\_queries}}\sphinxstyleindexpageref{mongoDB_query_generation:\detokenize{module-execute_queries}}
\indexspace
\bigletter{f}
\item {\sphinxstyleindexentry{flink\_api}}\sphinxstyleindexpageref{flink:\detokenize{module-flink_api}}
\item {\sphinxstyleindexentry{flink\_code\_gen}}\sphinxstyleindexpageref{flink:\detokenize{module-flink_code_gen}}
\indexspace
\bigletter{g}
\item {\sphinxstyleindexentry{generate\_queries}}\sphinxstyleindexpageref{neo4j_query_generation:\detokenize{module-generate_queries}}
\indexspace
\bigletter{i}
\item {\sphinxstyleindexentry{ingest\_neo4j\_streaming}}\sphinxstyleindexpageref{neo4j_data_ingestion:\detokenize{module-ingest_neo4j_streaming}}
\item {\sphinxstyleindexentry{ingest\_neo4j\_user\_timeline}}\sphinxstyleindexpageref{neo4j_data_ingestion:\detokenize{module-ingest_neo4j_user_timeline}}
\item {\sphinxstyleindexentry{ingest\_raw}}\sphinxstyleindexpageref{mongoDB_data_ingestion:\detokenize{module-ingest_raw}}
\indexspace
\bigletter{k}
\item {\sphinxstyleindexentry{kafka\_flink\_alerts\_consumer}}\sphinxstyleindexpageref{flink:\detokenize{module-kafka_flink_alerts_consumer}}
\item {\sphinxstyleindexentry{kafka\_tweets\_producer}}\sphinxstyleindexpageref{kafka:\detokenize{module-kafka_tweets_producer}}
\indexspace
\bigletter{q}
\item {\sphinxstyleindexentry{query\_answering}}\sphinxstyleindexpageref{benchmarking:\detokenize{module-query_answering}}
\indexspace
\bigletter{s}
\item {\sphinxstyleindexentry{streaming}}\sphinxstyleindexpageref{twitter_stream:\detokenize{module-streaming}}
\indexspace
\bigletter{u}
\item {\sphinxstyleindexentry{userstimeline}}\sphinxstyleindexpageref{twitter_stream:\detokenize{module-userstimeline}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}